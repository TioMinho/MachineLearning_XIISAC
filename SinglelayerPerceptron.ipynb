{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singlelayer Perceptron\n",
    "\n",
    "### Descrição:\n",
    "Nesse notebook iremos utilizar a arquitetura do <b>Singlelayer Perceptron</b> para realizar uma <i>multiclass classification</i> para o famoso problema do Iris-Dataset.\n",
    "\n",
    "O Singlelayer Perceptron, uma Rede Neural Artificial, possui exatamente os mesmos algoritmos de uma Regressão Logística. Para exemplificarmos seu potencial, desenvolvemos um problema de classificação multiclasse para expressar seu funcionamento e motivar a preparação para o Multilayer Perceptron. No problema de <i>multiclass classification</i>, devemos sempre nos preocupar em enumerar nossas classes e aplicar o <i>one-hot encoding</i>.\n",
    "\n",
    "<b> Obs.: </b> todas as matrizes/vetores utilizados na fundamentação teórica são consideradas como Vetores-Colunas. A implementação pode diferir um pouco dessa convenção.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "%matplotlib qt5\n",
    "\n",
    "# Libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Perceptron\n",
    "\n",
    "O Singlelayer Perceptron difere da Regressão Logística (em sua convenção) por utilizar uma diferente \"função logística\",  a <b>Função Sinal</b>, que possui a seguinte regra:\n",
    "\n",
    "$$\n",
    "    \\varphi(x) = \\begin{cases} 1 & \\text{ if } x \\geq 0 \\\\  0 & \\text{ if } x < 0 \\end{cases}\n",
    "$$\n",
    "\n",
    "No entanto, essa função possui algumas dificuldades de implementação, além de ser vulnerável ao problema de dupla-classificação. Logo, continuaremos utilizando a função logística Sigmoide como função ativadora.\n",
    "\n",
    "$$\n",
    "    \\varphi(x) = \\frac{1}{1+e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definição das Funções Sigmoide e Modelo Perceptron\n",
    "def sigmoid(X):\n",
    "    ''' Returns the Sigmoid Function applied to a vector (or value) x '''\n",
    "    return 1 / (1 + np.exp(-1 * X))\n",
    "    \n",
    "def h_theta(X, theta):\n",
    "    ''' Apply the Linear Model for features X and parameters theta '''\n",
    "    return sigmoid(np.dot(np.transpose(theta), X))\n",
    "\n",
    "# Teste da Sigmoide\n",
    "testSigmoid = np.linspace(-30, 30)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title(\"Sigmoid Function\")\n",
    "plt.plot(testSigmoid, sigmoid(testSigmoid), 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "Os Datasets não trarão as classes já configuradas em One-Hot Encoding. Ao invés disso, encontraremos muitos datasets que as classes serão enumeradas (de $1..k$). Esse é o caso do Iris-Dataset.\n",
    "\n",
    "Criaremos, então, uma função que receba tal enumeração e a transforme em uma matriz One-Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "def oneHotEncoding(y):\n",
    "    y = y.astype(int)\n",
    "    \n",
    "    classes = np.max(y)\n",
    "    examples = np.size(y,0)\n",
    "    oneHot = np.zeros([classes, examples])\n",
    "    \n",
    "    for j in range(examples):\n",
    "        oneHot[y[j]-1, j] = 1\n",
    "        \n",
    "    return oneHot\n",
    "        \n",
    "# Teste\n",
    "y = np.array([1, 3, 4, 5 ,1, 1, 2, 3, 4, 2, 5])\n",
    "\n",
    "y = oneHotEncoding(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de Predição\n",
    "\n",
    "No nosso modelo, nossas predições estarão no formato <i>one-hot encoding</i>. No entanto, como estamos usando a Sigmoide, nossas predições terão valores dentro do intervalo [0,1]. Para realizar a classificação corretamente, iremos escrever uma função que irá verifiar, para cada exemplar, qual classe ele terá mais probabilidade de pertencer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Prediction:\n",
      " [[ 0.93702664  0.0099518   0.21416502]\n",
      " [ 0.5         0.450166    0.80218389]\n",
      " [ 0.7109495   0.11920292  0.52497919]]\n",
      "\n",
      "After Prediction:\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  1.  1.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Definição da Função de OneHotEncoding\n",
    "def predictOHE(y):\n",
    "    classes = np.size(y,0)\n",
    "    examples = np.size(y,1)\n",
    "    \n",
    "    oneHot = np.zeros([classes, examples])\n",
    "    \n",
    "    for j in range(examples):\n",
    "        maxPos = np.argmax(y[:,j], axis=0)\n",
    "        oneHot[maxPos,j] = 1\n",
    "    \n",
    "    return oneHot\n",
    "    \n",
    "# Teste da Função de OneHotEncoding\n",
    "x_ohe = np.array([[-10, 10, 7],\n",
    "                  [  5, -8,-3],\n",
    "                  [  2,  0, 1]])\n",
    "\n",
    "theta_ohe = np.array([[0.1, 0.3, 0.2],\n",
    "                      [0.7, 0.4, 0.5],\n",
    "                      [0.1, 0.5, 0.2]])\n",
    "\n",
    "y_pred_ohe = h_theta(x_ohe, theta_ohe)\n",
    "print(\"Before Prediction:\\n\", y_pred_ohe)\n",
    "print(\"\")\n",
    "\n",
    "y_pred_ohe = predictOHE(y_pred_ohe)\n",
    "print(\"After Prediction:\\n\", y_pred_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de Acurácia\n",
    "\n",
    "Em problemas de Classificação, subtrair o valor previsto do valor real não nos dá uma informação muito precisa (sobre, por exemplo, a gravidade do erro). Para avaliar melhor a qualidade de um Treinamento de Classificação existem diversas medidas. Uma bem comum, e simples, consiste na acurácia: a quantidade de exemplos \"corretamente classificados\". Vamos definir essa porcentagem como sendo:\n",
    "\n",
    "$$ Acc(\\theta) = 100 \\times (1 - \\frac{1}{m} \\sum (h(\\theta) - y)^{2}) $$\n",
    "\n",
    "<b>Obs.:</b> a acurácia só será correta caso tanto $h(\\theta)$ quanto $y$ sejam valores binários. Logo, iremos arredondar os resultados de $h(\\theta)$ utilizando a função <b>Predict()</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracyFunction(X, y, theta):\n",
    "    ''' Calculates the percentage of correct classifications '''\n",
    "    Y_pred = predictOHE(h_theta(X, theta))\n",
    "    \n",
    "    return 100 * (1 - (1 / np.size(y)) * np.sum((Y_pred - y) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linha de Decisão\n",
    "\n",
    "Na célula abaixo iremos definir uma função simples para calcular e retornar os eixos cartesianos da Linha de Decisão estimada pelo modelo. Essa função apenas utiliza os parâmetros $\\theta$ para calcular o coeficiente angular e linear da reta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decisionBound(theta, x, attribute):\n",
    "    ''' Calculates and returns a linear Decision Boundary from the model '''\n",
    "    boundary_X = np.linspace(min(x[attribute,:]), max(x[attribute,:]))\n",
    "    boundary_Y = -1*(theta[1] / theta[2]) * boundary_X - (theta[0] / theta[2]);\n",
    "    return [boundary_X, boundary_Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração de Features\n",
    "\n",
    "Na célula abaixo, definimos a função que será responsável por extrair o conjunto de <i>features</i> de todo o Dataset. Utilizaremos essa função pois, com ela, podemos aumentar a complexidade do nosso modelo ao criar novos atributos que sejam transformações não-lineares dos <i>features</i> do Dataset.\n",
    "\n",
    "É importante notar que caso o Dataset já possua $n$ <i>features</i>, cada transformação será aplicada às $n$ <i>features</i> e incluida no conjunto, implicando num crescimento de atributos igual a $(Kn)$, onde $K$ representa o grau de complexidade do polinômio dessa transformação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with complexity 1:\n",
      " [[ 1.  1.  1.]\n",
      " [ 2.  5.  8.]\n",
      " [ 3.  6.  9.]]\n",
      "\n",
      "Features with complexity 3:\n",
      " [[   1.    1.    1.]\n",
      " [   2.    5.    8.]\n",
      " [   3.    6.    9.]\n",
      " [   4.   25.   64.]\n",
      " [   9.   36.   81.]\n",
      " [   8.  125.  512.]\n",
      " [  27.  216.  729.]]\n"
     ]
    }
   ],
   "source": [
    "# Definição da Função de Extração\n",
    "def featureExtraction(data, n_examples, complexity=1):\n",
    "    ''' Extracts the features from a dataset and apply polynomial transformations '''\n",
    "    x = np.ones(n_examples)\n",
    "    \n",
    "    for i in range(0, complexity):\n",
    "        x = np.vstack([x, np.transpose(data[:, 0:-1]) ** (i+1)])\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Teste da Função\n",
    "dataExtract = np.array([[2, 3, 0],\n",
    "                        [5, 6, 0],\n",
    "                        [8, 9, 0]])\n",
    "\n",
    "x_extract = featureExtraction(dataExtract, 3, 1)\n",
    "print(\"Features with complexity 1:\\n\", x_extract)\n",
    "print(\"\")\n",
    "\n",
    "x_extract = featureExtraction(dataExtract, 3, 3)\n",
    "print(\"Features with complexity 3:\\n\", x_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Na célula abaixo criamos uma função para <b>normalizar</b> as nossas <i>features</i>. O objetivo, como mostrado, é evitar problemas de divergência e permitir um melhor treinamento por meio dos hiperparâmetros.\n",
    "\n",
    "A fórmula que usaremos para o <i>Feature Scaling</i> será:\n",
    "\n",
    "$$\n",
    "    X = \\frac{X - \\bar{X}}{\\sigma(X)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features normalizados:\n",
      " [[ 1.          1.          1.          1.          1.        ]\n",
      " [-1.40693001  1.4896906  -0.57932412  0.66208471 -0.16552118]\n",
      " [ 0.23284516  0.62092042  1.39707095 -1.31945589 -0.93138063]]\n"
     ]
    }
   ],
   "source": [
    "# Definição da Função de Normalização\n",
    "def normalizeData(data):\n",
    "    ''' Apply Feature Scaling to the features set '''\n",
    "    for i in range(1, np.size(data,0)):\n",
    "        data[i,:] = (data[i,:]-np.mean(data[i,:])) / np.std(data[i,:])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Teste da Função\n",
    "x_norm = np.array([[1., 1., 1., 1., 1.],\n",
    "                   [2., 9., 4., 7., 5.],\n",
    "                   [-5.,-4.,-2.,-9.,-8.]])\n",
    "\n",
    "x_norm = normalizeData(x_norm)\n",
    "print(\"Features normalizados:\\n\",x_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programa Principal (Regressão Linear)\n",
    "\n",
    "No programa principal, iremos programar a Regressão Linear propriamente dita.\n",
    "Dividimos o código em três partes:\n",
    "\n",
    "### Part 1: Data Pre-Processing\n",
    "\n",
    "Nesse trecho, iremos nos preocupar em carregar e organizar o dataset que utilizaremos no treinamento. É nesse momento, também, que iremos declarar e separar as variáveis que definem o Conjunto de Atributos, o Conjunto de Saída e os Parâmetros do Modelo, além dos Hiperparâmetros de Treinamento. Iremos seguir a convenção de considerar todos os exemplares como vetores-colunas. No entanto, o numpy não nos permite facilmente modificar essa informação para o Conjunto de Saída, e o mesmo continuará como vetor-linha (sem muito prejuízo). Iremos criar, também um vetor para armazenar o Histórico de Erros do treinamento (por motivos de visualização).\n",
    "\n",
    "Iremos utilizar o dataset <i>data1.txt</i> localizado na pasta <i>datasets/</i>. Teremos as seguintes matrizes:\n",
    "\n",
    "$$\n",
    "    X = \\begin{bmatrix} 1 & 1 & \\cdots & 1 \\\\  X_{1}^{(1)} & X_{1}^{(2)} & \\cdots & X_{1}^{(m)}  \\end{bmatrix};\\   \\theta = \\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1}\\end{bmatrix};\\  Y = \\begin{bmatrix} Y^{(1)} & Y^{(2)} & \\cdots & Y^{(m)} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Part 2: Linear Regression Training\n",
    "\n",
    "Para cada época até a convergência (ou até atingir o limite máximo definido pelo Hiperparâmetro) iremos realizar o Treinamento da Regressão Linear. Os passos serão os seguintes:\n",
    "\n",
    "1. Calculamos o vetor de predição \"Y_pred\", como resultado da predição do Modelo para os parâmetros daquela época;\n",
    "2. Utilizando \"Y_pred\", calculamos os erros de acordo com o a matriz real \"Y\";\n",
    "3. Concatenamos o Custo Total do erro calculado no Histórico de Erros;\n",
    "4. Realizamos, para cada parâmetro, o Gradiente Descendente para estimar os novos valores dos parâmetros;\n",
    "5. Imprimimos os resultados do treino a cada 500 épocas;\n",
    "6. Verificamos uma possível convergência do treino, e paremos o mesmo caso seja verificado;\n",
    "\n",
    "### Part 3: Data Plotting and Training Results\n",
    "\n",
    "Ao fim do treinamento, iremos plotar duas figuras para avaliar o resultado final do nosso algoritmo. A <b>Figura 1</b> irá apenas exibir os atributos do Dataset. A <b>Figura 2</b> irá exibir a função estimada pelo nosso Modelo Linear, além do Histórico de Erros dado as épocas até convergência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Epoch 0 ######\n",
      "Error: 55.5555555556\n",
      "Thetas:\n",
      " [[-0.16666667 -0.16666667 -0.16666667]\n",
      " [-0.33819299  0.03742741  0.30076558]\n",
      " [ 0.28076893 -0.21906147 -0.06170746]\n",
      " [-0.43495945  0.09502891  0.33993054]\n",
      " [-0.41837621  0.05580297  0.36257324]]\n",
      "\n",
      "###### Epoch 50 ######\n",
      "Error: 93.3333333333\n",
      "Thetas:\n",
      " [[-1.63593904 -0.95031297 -2.28894432]\n",
      " [-0.95666636  0.20070582  0.29113115]\n",
      " [ 1.55881742 -1.37575248  0.01253778]\n",
      " [-1.73634731  0.43342094  1.51802827]\n",
      " [-1.60662737 -0.6117253   2.28765224]]\n",
      "\n",
      "###### Epoch 100 ######\n",
      "Error: 95.1111111111\n",
      "Thetas:\n",
      " [[-2.01902475 -0.96558348 -3.17360766]\n",
      " [-1.08477329  0.24901582  0.05105699]\n",
      " [ 1.81429929 -1.40270191 -0.22051725]\n",
      " [-2.05895068  0.68781721  2.07983236]\n",
      " [-1.89514853 -0.93091344  3.14382919]]\n",
      "\n",
      "###### Epoch 150 ######\n",
      "Error: 96.4444444444\n",
      "Thetas:\n",
      " [[-2.25461653 -0.97113798 -3.81711812]\n",
      " [-1.16631092  0.21185949 -0.08478876]\n",
      " [ 1.95654204 -1.38438933 -0.38559613]\n",
      " [-2.26531234  0.91766238  2.51380122]\n",
      " [-2.07938112 -1.12541263  3.72051392]]\n",
      "\n",
      "Gradient Converged!!!\n",
      "Stopping at epoch 185\n",
      "###### Epoch 185 ######\n",
      "Error: 96.4444444444\n",
      "Thetas:\n",
      " [[-2.37999632 -0.97375984 -4.18405989]\n",
      " [-1.20990676  0.1745405  -0.152196  ]\n",
      " [ 2.02805603 -1.3667642  -0.47453906]\n",
      " [-2.37695934  1.06004619  2.77187834]\n",
      " [-2.17898995 -1.22939106  4.03335576]]\n",
      "\n",
      "Results:\n",
      "### Predictions ###\n",
      "\n",
      "Class 0\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "Class 1\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.\n",
      "  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.\n",
      "  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  0.  1.  0.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "Class 2\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.]\n",
      "\n",
      "### Real ###\n",
      "\n",
      "Class 0\n",
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "Class 1\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "Class 2\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Main Function\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    ###############################\n",
    "    # Part 1: Data Pre-Processing #\n",
    "    ###############################\n",
    "    # Loads the data\n",
    "    data = np.loadtxt(\"datasets/irisDataset.txt\")\n",
    "    \n",
    "    n_examples = np.size(data, 0)\n",
    "    \n",
    "    # Define the model parameters\n",
    "    x = normalizeData(featureExtraction(data, n_examples, 1))    \n",
    "    y = oneHotEncoding(data[:, -1])\n",
    "    \n",
    "    n_features = np.size(x, 0)\n",
    "    n_classes = np.size(y, 0)\n",
    "    \n",
    "    theta = np.zeros([n_features, n_classes])\n",
    "\n",
    "    # Defines the hyperparameters and training measurements\n",
    "    alfa = 1\n",
    "    max_epochs = 500000\n",
    "    \n",
    "    error_hist = np.zeros([max_epochs])\n",
    "    epsilon = 0.001\n",
    "    \n",
    "    ######################################\n",
    "    # Part 2: Linear Regression Training #\n",
    "    ######################################\n",
    "    for epochs in range(max_epochs):\n",
    "        # Calculate the error vector from the current Model\n",
    "        y_pred = h_theta(x, theta)\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # Append new Least Square Error to History\n",
    "        error_hist[epochs] = accuracyFunction(x, y, theta)\n",
    "\n",
    "        # Perform Gradient Descent\n",
    "        for k in range(n_classes):\n",
    "            for j in range(n_features):\n",
    "                theta[j,k] = theta[j,k] - (alfa/n_examples) * np.sum(error[k,:] * x[j,:])\n",
    "\n",
    "        # Prints training status at each 100 epochs\n",
    "        if(epochs % 50 == 0):\n",
    "            print(\"###### Epoch\", epochs, \"######\")\n",
    "            print(\"Error:\", error_hist[epochs])\n",
    "            print(\"Thetas:\\n\", theta)\n",
    "            print(\"\")\n",
    "        \n",
    "        # Evaluate convergence and stops training if so\n",
    "        if(abs(error_hist[epochs] - error_hist[epochs-50]) <= epsilon):\n",
    "            print(\"Gradient Converged!!!\\nStopping at epoch\", epochs)\n",
    "            print(\"###### Epoch\", epochs, \"######\")\n",
    "            print(\"Error:\", error_hist[epochs])\n",
    "            print(\"Thetas:\\n\", theta)\n",
    "            print(\"\")\n",
    "            break\n",
    "\n",
    "    ######################################\n",
    "    # Part 3: Visualizing Classification #\n",
    "    ######################################\n",
    "    predictions = predictOHE(h_theta(x, theta))\n",
    "    \n",
    "    print(\"Results:\")\n",
    "    print(\"### Predictions ###\")\n",
    "    for i,j in zip(predictions, range(n_classes)):\n",
    "        print(\"\\nClass\",j)\n",
    "        print(i)\n",
    "        \n",
    "    print(\"\\n### Real ###\")\n",
    "    for i,j in zip(y, range(n_classes)):\n",
    "        print(\"\\nClass\",j)\n",
    "        print(i)\n",
    "#__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotagem dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Part 3: Data Plotting and Training Result #\n",
    "#############################################\n",
    "# First Figure: Dataset plotting\n",
    "iSetos = np.where(y[0,:] == 1)\n",
    "iVersi = np.where(y[1,:] == 1)\n",
    "iVirg = np.where(y[2,:] == 1)\n",
    "\n",
    "plt.figure(2)\n",
    "plotCount = 1\n",
    "for i in range(1,4):\n",
    "    for j in range(i+1,5):\n",
    "        plt.subplot(2,3,plotCount)\n",
    "        plt.title(\"Iris Dataset Classification\\n(Green=Iris-setosa; Blue=Iris-versicolor; Red=Iris-virginica)\")\n",
    "        plt.grid()\n",
    "        plt.plot(x[i,iSetos], x[j,iSetos], 'go', x[i,iVersi], x[j,iVersi], 'bo', x[i,iVirg], x[j,iVirg], 'ro')\n",
    "        plotCount += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
