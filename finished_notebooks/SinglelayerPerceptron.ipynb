{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singlelayer Perceptron\n",
    "\n",
    "### Descrição:\n",
    "Nesse notebook iremos utilizar a arquitetura do <b>Singlelayer Perceptron</b> para realizar uma <i>multiclass classification</i> para o famoso problema do Iris-Dataset.\n",
    "\n",
    "O Singlelayer Perceptron, uma Rede Neural Artificial, possui exatamente os mesmos algoritmos de uma Regressão Logística. Para exemplificarmos seu potencial, desenvolvemos um problema de classificação multiclasse para expressar seu funcionamento e motivar a preparação para o Multilayer Perceptron. No problema de <i>multiclass classification</i>, devemos sempre nos preocupar em enumerar nossas classes e aplicar o <i>one-hot encoding</i>.\n",
    "\n",
    "<b> Obs.: </b> todas as matrizes/vetores utilizados na fundamentação teórica são consideradas como Vetores-Colunas. A implementação pode diferir um pouco dessa convenção.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "%matplotlib qt5\n",
    "\n",
    "# Libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Perceptron\n",
    "\n",
    "O Singlelayer Perceptron difere da Regressão Logística (em sua convenção) por utilizar uma diferente \"função logística\",  a <b>Função Sinal</b>, que possui a seguinte regra:\n",
    "\n",
    "$$\n",
    "    \\varphi(x) = \\begin{cases} 1 & \\text{ if } x \\geq 0 \\\\  0 & \\text{ if } x < 0 \\end{cases}\n",
    "$$\n",
    "\n",
    "No entanto, essa função possui algumas dificuldades de implementação, além de ser vulnerável ao problema de dupla-classificação. Logo, continuaremos utilizando a função logística Sigmoide como função ativadora.\n",
    "\n",
    "$$\n",
    "    \\varphi(x) = \\frac{1}{1+e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definição das Funções Sigmoide e Modelo Perceptron\n",
    "def sigmoid(X):\n",
    "    ''' Returns the Sigmoid Function applied to a vector (or value) x '''\n",
    "    return 1 / (1 + np.exp(-1 * X))\n",
    "    \n",
    "def h_theta(X, theta):\n",
    "    ''' Apply the Linear Model for features X and parameters theta '''\n",
    "    return sigmoid(np.dot(np.transpose(theta), X))\n",
    "\n",
    "# Teste da Sigmoide\n",
    "testSigmoid = np.linspace(-30, 30)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title(\"Sigmoid Function\")\n",
    "plt.plot(testSigmoid, sigmoid(testSigmoid), 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "Os Datasets não trarão as classes já configuradas em One-Hot Encoding. Ao invés disso, encontraremos muitos datasets que as classes serão enumeradas (de $1..k$). Esse é o caso do Iris-Dataset.\n",
    "\n",
    "Criaremos, então, uma função que receba tal enumeração e a transforme em uma matriz One-Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def oneHotEncoding(y):\n",
    "    y = y.astype(int)\n",
    "    \n",
    "    classes = np.max(y)\n",
    "    examples = np.size(y,0)\n",
    "    oneHot = np.zeros([classes, examples])\n",
    "    \n",
    "    for j in range(examples):\n",
    "        oneHot[y[j]-1, j] = 1\n",
    "        \n",
    "    return oneHot\n",
    "        \n",
    "# Teste\n",
    "y = np.array([1, 3, 4, 5 ,1, 1, 2, 3, 4, 2, 5])\n",
    "\n",
    "y = oneHotEncoding(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de Predição\n",
    "\n",
    "No nosso modelo, nossas predições estarão no formato <i>one-hot encoding</i>. No entanto, como estamos usando a Sigmoide, nossas predições terão valores dentro do intervalo [0,1]. Para realizar a classificação corretamente, iremos escrever uma função que irá verifiar, para cada exemplar, qual classe ele terá mais probabilidade de pertencer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definição da Função de OneHotEncoding\n",
    "def predictOHE(y):\n",
    "    classes = np.size(y,0)\n",
    "    examples = np.size(y,1)\n",
    "    \n",
    "    oneHot = np.zeros([classes, examples])\n",
    "    \n",
    "    for j in range(examples):\n",
    "        maxPos = np.argmax(y[:,j], axis=0)\n",
    "        oneHot[maxPos,j] = 1\n",
    "    \n",
    "    return oneHot\n",
    "    \n",
    "# Teste da Função de OneHotEncoding\n",
    "x_ohe = np.array([[-10, 10, 7],\n",
    "                  [  5, -8,-3],\n",
    "                  [  2,  0, 1]])\n",
    "\n",
    "theta_ohe = np.array([[0.1, 0.3, 0.2],\n",
    "                      [0.7, 0.4, 0.5],\n",
    "                      [0.1, 0.5, 0.2]])\n",
    "\n",
    "y_pred_ohe = h_theta(x_ohe, theta_ohe)\n",
    "print(\"Before Prediction:\\n\", y_pred_ohe)\n",
    "print(\"\")\n",
    "\n",
    "y_pred_ohe = predictOHE(y_pred_ohe)\n",
    "print(\"After Prediction:\\n\", y_pred_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de Acurácia\n",
    "\n",
    "Em problemas de Classificação, subtrair o valor previsto do valor real não nos dá uma informação muito precisa (sobre, por exemplo, a gravidade do erro). Para avaliar melhor a qualidade de um Treinamento de Classificação existem diversas medidas. Uma bem comum, e simples, consiste na acurácia: a quantidade de exemplos \"corretamente classificados\". Vamos definir essa porcentagem como sendo:\n",
    "\n",
    "$$ Acc(\\theta) = 100 \\times (1 - \\frac{1}{m} \\sum (h(\\theta) - y)^{2}) $$\n",
    "\n",
    "<b>Obs.:</b> a acurácia só será correta caso tanto $h(\\theta)$ quanto $y$ sejam valores binários. Logo, iremos arredondar os resultados de $h(\\theta)$ utilizando a função <b>Predict()</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracyFunction(X, y, theta):\n",
    "    ''' Calculates the percentage of correct classifications '''\n",
    "    Y_pred = predictOHE(h_theta(X, theta))\n",
    "    \n",
    "    return 100 * (1 - (1 / np.size(y)) * np.sum((Y_pred - y) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração de Features\n",
    "\n",
    "Na célula abaixo, definimos a função que será responsável por extrair o conjunto de <i>features</i> de todo o Dataset. Utilizaremos essa função pois, com ela, podemos aumentar a complexidade do nosso modelo ao criar novos atributos que sejam transformações não-lineares dos <i>features</i> do Dataset.\n",
    "\n",
    "É importante notar que caso o Dataset já possua $n$ <i>features</i>, cada transformação será aplicada às $n$ <i>features</i> e incluida no conjunto, implicando num crescimento de atributos igual a $(Kn)$, onde $K$ representa o grau de complexidade do polinômio dessa transformação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definição da Função de Extração\n",
    "def featureExtraction(data, n_examples, complexity=1):\n",
    "    ''' Extracts the features from a dataset and apply polynomial transformations '''\n",
    "    x = np.ones(n_examples)\n",
    "    \n",
    "    for i in range(0, complexity):\n",
    "        x = np.vstack([x, np.transpose(data[:, 0:-1]) ** (i+1)])\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Teste da Função\n",
    "dataExtract = np.array([[2, 3, 0],\n",
    "                        [5, 6, 0],\n",
    "                        [8, 9, 0]])\n",
    "\n",
    "x_extract = featureExtraction(dataExtract, 3, 1)\n",
    "print(\"Features with complexity 1:\\n\", x_extract)\n",
    "print(\"\")\n",
    "\n",
    "x_extract = featureExtraction(dataExtract, 3, 3)\n",
    "print(\"Features with complexity 3:\\n\", x_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Na célula abaixo criamos uma função para <b>normalizar</b> as nossas <i>features</i>. O objetivo, como mostrado, é evitar problemas de divergência e permitir um melhor treinamento por meio dos hiperparâmetros.\n",
    "\n",
    "A fórmula que usaremos para o <i>Feature Scaling</i> será:\n",
    "\n",
    "$$\n",
    "    X = \\frac{X - \\bar{X}}{\\sigma(X)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definição da Função de Normalização\n",
    "def normalizeData(data):\n",
    "    ''' Apply Feature Scaling to the features set '''\n",
    "    for i in range(1, np.size(data,0)):\n",
    "        data[i,:] = (data[i,:]-np.mean(data[i,:])) / np.std(data[i,:])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Teste da Função\n",
    "x_norm = np.array([[1., 1., 1., 1., 1.],\n",
    "                   [2., 9., 4., 7., 5.],\n",
    "                   [-5.,-4.,-2.,-9.,-8.]])\n",
    "\n",
    "x_norm = normalizeData(x_norm)\n",
    "print(\"Features normalizados:\\n\",x_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programa Principal (Regressão Linear)\n",
    "\n",
    "No programa principal, iremos programar a Regressão Linear propriamente dita.\n",
    "Dividimos o código em três partes:\n",
    "\n",
    "### Part 1: Data Pre-Processing\n",
    "\n",
    "Nesse trecho, iremos nos preocupar em carregar e organizar o dataset que utilizaremos no treinamento. É nesse momento, também, que iremos declarar e separar as variáveis que definem o Conjunto de Atributos, o Conjunto de Saída e os Parâmetros do Modelo, além dos Hiperparâmetros de Treinamento. Iremos seguir a convenção de considerar todos os exemplares como vetores-colunas. No entanto, o numpy não nos permite facilmente modificar essa informação para o Conjunto de Saída, e o mesmo continuará como vetor-linha (sem muito prejuízo). Iremos criar, também um vetor para armazenar o Histórico de Erros do treinamento (por motivos de visualização).\n",
    "\n",
    "Iremos utilizar o dataset <i>data1.txt</i> localizado na pasta <i>datasets/</i>. Teremos as seguintes matrizes:\n",
    "\n",
    "$$\n",
    "    X = \\begin{bmatrix} 1 & 1 & \\cdots & 1 \\\\  X_{1}^{(1)} & X_{1}^{(2)} & \\cdots & X_{1}^{(m)}  \\end{bmatrix};\\   \\theta = \\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1}\\end{bmatrix};\\  Y = \\begin{bmatrix} Y^{(1)} & Y^{(2)} & \\cdots & Y^{(m)} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Part 2: Linear Regression Training\n",
    "\n",
    "Para cada época até a convergência (ou até atingir o limite máximo definido pelo Hiperparâmetro) iremos realizar o Treinamento da Regressão Linear. Os passos serão os seguintes:\n",
    "\n",
    "1. Calculamos o vetor de predição \"Y_pred\", como resultado da predição do Modelo para os parâmetros daquela época;\n",
    "2. Utilizando \"Y_pred\", calculamos os erros de acordo com o a matriz real \"Y\";\n",
    "3. Concatenamos o Custo Total do erro calculado no Histórico de Erros;\n",
    "4. Realizamos, para cada parâmetro, o Gradiente Descendente para estimar os novos valores dos parâmetros;\n",
    "5. Imprimimos os resultados do treino a cada 500 épocas;\n",
    "6. Verificamos uma possível convergência do treino, e paremos o mesmo caso seja verificado;\n",
    "\n",
    "### Part 3: Data Plotting and Training Results\n",
    "\n",
    "Ao fim do treinamento, iremos plotar duas figuras para avaliar o resultado final do nosso algoritmo. A <b>Figura 1</b> irá apenas exibir os atributos do Dataset. A <b>Figura 2</b> irá exibir a função estimada pelo nosso Modelo Linear, além do Histórico de Erros dado as épocas até convergência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Main Function\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    ###############################\n",
    "    # Part 1: Data Pre-Processing #\n",
    "    ###############################\n",
    "    # Loads the data\n",
    "    data = np.loadtxt(\"datasets/irisDataset.txt\")\n",
    "    \n",
    "    n_examples = np.size(data, 0)\n",
    "    \n",
    "    # Define the model parameters\n",
    "    x = normalizeData(featureExtraction(data, n_examples, 1))    \n",
    "    y = oneHotEncoding(data[:, -1])\n",
    "    \n",
    "    n_features = np.size(x, 0)\n",
    "    n_classes = np.size(y, 0)\n",
    "    \n",
    "    theta = np.zeros([n_features, n_classes])\n",
    "\n",
    "    # Defines the hyperparameters and training measurements\n",
    "    alfa = 1\n",
    "    max_epochs = 500000\n",
    "    \n",
    "    error_hist = np.zeros([max_epochs])\n",
    "    epsilon = 0.001\n",
    "    \n",
    "    ######################################\n",
    "    # Part 2: Linear Regression Training #\n",
    "    ######################################\n",
    "    for epochs in range(max_epochs):\n",
    "        # Calculate the error vector from the current Model\n",
    "        y_pred = h_theta(x, theta)\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # Append new Least Square Error to History\n",
    "        error_hist[epochs] = accuracyFunction(x, y, theta)\n",
    "\n",
    "        # Perform Gradient Descent\n",
    "        for k in range(n_classes):\n",
    "            for j in range(n_features):\n",
    "                theta[j,k] = theta[j,k] - (alfa/n_examples) * np.sum(error[k,:] * x[j,:])\n",
    "\n",
    "        # Prints training status at each 100 epochs\n",
    "        if(epochs % 50 == 0):\n",
    "            print(\"###### Epoch\", epochs, \"######\")\n",
    "            print(\"Error:\", error_hist[epochs])\n",
    "            print(\"Thetas:\\n\", theta)\n",
    "            print(\"\")\n",
    "        \n",
    "        # Evaluate convergence and stops training if so\n",
    "        if(abs(error_hist[epochs] - error_hist[epochs-50]) <= epsilon):\n",
    "            print(\"Gradient Converged!!!\\nStopping at epoch\", epochs)\n",
    "            print(\"###### Epoch\", epochs, \"######\")\n",
    "            print(\"Error:\", error_hist[epochs])\n",
    "            print(\"Thetas:\\n\", theta)\n",
    "            print(\"\")\n",
    "            break\n",
    "\n",
    "    ######################################\n",
    "    # Part 3: Visualizing Classification #\n",
    "    ######################################\n",
    "    predictions = predictOHE(h_theta(x, theta))\n",
    "    \n",
    "    print(\"Results:\")\n",
    "    print(\"### Predictions ###\")\n",
    "    for i,j in zip(predictions, range(n_classes)):\n",
    "        print(\"\\nClass\",j)\n",
    "        print(i)\n",
    "        \n",
    "    print(\"\\n### Real ###\")\n",
    "    for i,j in zip(y, range(n_classes)):\n",
    "        print(\"\\nClass\",j)\n",
    "        print(i)\n",
    "#__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotagem dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Part 3: Data Plotting and Training Result #\n",
    "#############################################\n",
    "# First Figure: Dataset plotting\n",
    "iSetos = np.where(y[0,:] == 1)\n",
    "iVersi = np.where(y[1,:] == 1)\n",
    "iVirg = np.where(y[2,:] == 1)\n",
    "\n",
    "plt.figure(2)\n",
    "plotCount = 1\n",
    "for i in range(1,4):\n",
    "    for j in range(i+1,5):\n",
    "        plt.subplot(2,3,plotCount)\n",
    "        plt.title(\"Iris Dataset Classification\\n(Green=Iris-setosa; Blue=Iris-versicolor; Red=Iris-virginica)\")\n",
    "        plt.grid()\n",
    "        plt.plot(x[i,iSetos], x[j,iSetos], 'go', x[i,iVersi], x[j,iVersi], 'bo', x[i,iVirg], x[j,iVirg], 'ro')\n",
    "        plotCount += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
