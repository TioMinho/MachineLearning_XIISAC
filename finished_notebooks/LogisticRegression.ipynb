{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logística Simples\n",
    "\n",
    "### Descrição:\n",
    "Nesse notebook iremos desenvolver e executar um algoritmo de Regressão Logística. Essa técnica é uma das mais simples técnicas de Classificação, e é extremamente similar à Regressão Linear. Utilizaremos uma versão simplificada do famoso Iris Dataset (Classificação de Flores).\n",
    "\n",
    "A Regressão Logística irá estimar parâmetros para um Modelo de Classificação. Isto é, o nosso modelo não irá prever um valor contínuo, como era na Regressão Linear. Ao invés disso, as predições irão informar uma probabilidade de tal exemplar pertencer (ou não) a determinada classe já estabelecida. O treinamento, então, busca determinar a chamada \"Linha de Decisão\", que separa linearmente quais exemplares pertencem a uma classe e quais não pertencem.\n",
    "\n",
    "<b> Obs.: </b> todas as matrizes/vetores utilizados na fundamentação teórica são consideradas como Vetores-Colunas. A implementação pode diferir um pouco dessa convenção.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "%matplotlib qt5\n",
    "\n",
    "# Libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import random as rnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Logístico\n",
    "\n",
    "Na célula abaixo, iremos programar nosso Modelo Logístico. Diferentemente da Regressão Linear, o nosso modelo depende de uma função não-linear (a própria função logística). Trata-se, justamente, da Sigmoide, definida da seguinte forma:\n",
    "\n",
    "$$ g(X) = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "Dessa forma, o nosso modelo final será\n",
    "\n",
    "$$ h(\\theta) = g(\\theta^{T}X) = \\frac{1}{1+e^{-\\theta^{T}X}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definição das Funções Sigmoide e Modelo Logístico\n",
    "def sigmoid(X):\n",
    "    ''' Returns the Sigmoid Function applied to a vector (or value) x '''\n",
    "    return 1 / (1 + np.exp(-1 * X))\n",
    "    \n",
    "def h_theta(X, theta):\n",
    "    ''' Apply the Linear Model for features X and parameters theta '''\n",
    "    return sigmoid(np.dot(np.transpose(theta), X))\n",
    "\n",
    "# Teste da Sigmoide\n",
    "testSigmoid = np.linspace(-30, 30)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title(\"Sigmoid Function\")\n",
    "plt.plot(testSigmoid, sigmoid(testSigmoid), 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de Acurácia\n",
    "\n",
    "Em problemas de Classificação, subtrair o valor previsto do valor real não nos dá uma informação muito precisa (sobre, por exemplo, a gravidade do erro). Para avaliar melhor a qualidade de um Treinamento de Classificação existem diversas medidas. Uma bem comum, e simples, consiste na acurácia: a quantidade de exemplos \"corretamente classificados\". Vamos definir essa porcentagem como sendo:\n",
    "\n",
    "$$ Acc(\\theta) = 100 \\times (1 - \\frac{1}{m} \\sum (h(\\theta) - y)^{2}) $$\n",
    "\n",
    "<b>Obs.:</b> a acurácia só será correta caso tanto $h(\\theta)$ quanto $y$ sejam valores binários. Logo, iremos arredondar os resultados de $h(\\theta)$. Os valores de $y$ já são binários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracyFunction(X, y, theta):\n",
    "    ''' Calculates the percentage of correct classifications '''\n",
    "    Y_pred = h_theta(X, theta)\n",
    "    pos = np.where(Y_pred >= 0.5); Y_pred[pos] = 1\n",
    "    neg = np.where(Y_pred < 0.5); Y_pred[neg] = 0\n",
    "    \n",
    "    return 100 * (1 - (1 / np.size(y)) * np.sum((Y_pred - y) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linha de Decisão\n",
    "\n",
    "Na célula abaixo iremos definir uma função simples para calcular e retornar os eixos cartesianos da Linha de Decisão estimada pelo modelo. Essa função apenas utiliza os parâmetros $\\theta$ para calcular o coeficiente angular e linear da reta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decisionBound(theta, x):\n",
    "    ''' Calculates and returns a linear Decision Boundary from the model '''\n",
    "    boundary_X = np.linspace(min(x[1,:]), max(x[1,:]))\n",
    "    boundary_Y = -1*(theta[1] / theta[2]) * boundary_X - (theta[0] / theta[2]);\n",
    "    return [boundary_X, boundary_Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programa Principal (Regressão Logística)\n",
    "\n",
    "No programa principal, iremos programar a Regressão Logística propriamente dita.\n",
    "O treinamento é <b>exatamente o mesmo</b> da Regressão Linear.\n",
    "Dividimos o código em três partes:\n",
    "\n",
    "### Part 1: Data Pre-Processing\n",
    "\n",
    "Nesse trecho, iremos nos preocupar em carregar e organizar o dataset que utilizaremos no treinamento. É nesse momento, também, que iremos declarar e separar as variáveis que definem o Conjunto de Atributos, o Conjunto de Saída e os Parâmetros do Modelo, além dos Hiperparâmetros de Treinamento. Iremos seguir a convenção de considerar todos os exemplares como vetores-colunas. No entanto, o numpy não nos permite facilmente modificar essa informação para o Conjunto de Saída, e o mesmo continuará como vetor-linha (sem muito prejuízo). Iremos criar, também um vetor para armazenar o Histórico de Erros do treinamento (por motivos de visualização).\n",
    "\n",
    "Iremos utilizar o dataset <i>irisDataSimple.txt</i> localizado na pasta <i>datasets/</i>. Teremos as seguintes matrizes:\n",
    "\n",
    "$$\n",
    "    X = \\begin{bmatrix} 1 & 1 & \\cdots & 1 \\\\  X_{1}^{(1)} & X_{1}^{(2)} & \\cdots & X_{1}^{(m)} \\\\  X_{2}^{(1)} & X_{2}^{(2)} & \\cdots & X_{2}^{(m)} \\end{bmatrix};\\   \\theta = \\begin{bmatrix} \\theta_{0}\\\\ \\theta_{1} \\\\ \\theta_{2}\\end{bmatrix};\\  Y = \\begin{bmatrix} Y^{(1)} & Y^{(2)} & \\cdots & Y^{(m)} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Part 2: Linear Regression Training\n",
    "\n",
    "Para cada época até a convergência (ou até atingir o limite máximo definido pelo Hiperparâmetro) iremos realizar o Treinamento da Regressão Linear. Os passos serão os seguintes:\n",
    "\n",
    "1. Calculamos o vetor de predição \"Y_pred\", como resultado da predição do Modelo para os parâmetros daquela época;\n",
    "2. Utilizando \"Y_pred\", calculamos os erros de acordo com o a matriz real \"Y\";\n",
    "3. Concatenamos o Custo Total do erro calculado no Histórico de Erros;\n",
    "4. Realizamos, para cada parâmetro, o Gradiente Descendente para estimar os novos valores dos parâmetros;\n",
    "5. Imprimimos os resultados do treino a cada 10 épocas;\n",
    "6. Verificamos uma possível convergência do treino, e paremos o mesmo caso seja verificado;\n",
    "\n",
    "### Part 3: Data Plotting and Training Results\n",
    "\n",
    "Ao fim do treinamento, iremos plotar duas figuras para avaliar o resultado final do nosso algoritmo. A <b>Figura 1</b> irá apenas exibir os atributos do Dataset. A <b>Figura 2</b> irá exibir a Linha de Decisão, além do Histórico de Erros dado as épocas até convergência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (100) into shape (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-30606aac486f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malfa\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_examples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Prints training status at each 50 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (100) into shape (1)"
     ]
    }
   ],
   "source": [
    "#  Main Function\n",
    "if __name__=='__main__':\n",
    "   \n",
    "    ###############################\n",
    "    # Part 1: Data Pre-Processing #\n",
    "    ###############################\n",
    "    # Loads the data\n",
    "    data = np.loadtxt(\"../datasets/irisDataSimple.txt\")\n",
    "    \n",
    "    n_examples = np.size(data,0)\n",
    "    n_features = np.size(data,1)\n",
    "    \n",
    "    # Define the model parameters\n",
    "    x = np.array([np.ones(n_examples), data[:, 0], data[:, 1]])\n",
    "    y = data[:, -1]\n",
    "    theta = np.zeros([np.size(x, 0), 1])\n",
    "    \n",
    "    # Defines the hyperparameters and training measurements\n",
    "    alfa = 0.5\n",
    "    max_epochs = 50000\n",
    "    \n",
    "    error_hist = np.zeros([max_epochs])\n",
    "    epsilon = 0.01\n",
    "    \n",
    "    ######################################\n",
    "    # Part 2: Linear Regression Training #\n",
    "    ######################################\n",
    "    for epochs in range(max_epochs):\n",
    "        # Randomly shuffle the data\n",
    "        randomIndex = rnd.sample(range(n_examples), n_examples)\n",
    "        x = x[:, randomIndex]\n",
    "        y = y[randomIndex]\n",
    "        \n",
    "        # Calculate the error vector from the current Model\n",
    "        y_pred = h_theta(x, theta)\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # Append new Least Square Error to History\n",
    "        error_hist[epochs] = accuracyFunction(x, y, theta)\n",
    "\n",
    "        # Perform Stochastic Gradient Descent\n",
    "        for j in range(n_features):\n",
    "            for k in range(n_examples):\n",
    "                theta[j] = theta[j] - (alfa/n_examples) * (error[0,k] * x[j,k])\n",
    "\n",
    "        # Prints training status at each 50 epochs    \n",
    "        if(epochs % 10 == 0):\n",
    "            print(\"###### Epoch\", epochs, \"######\")\n",
    "            print(\"Error:\", error_hist[epochs], \"%\")\n",
    "            print(\"Thetas:\\n\", theta)\n",
    "            print(\"\")\n",
    "        \n",
    "        # Evaluate convergence and stops training if so\n",
    "        if(abs(error_hist[epochs] - error_hist[epochs-50]) <= epsilon):\n",
    "            print(\"Gradient Converged!!!\\nStopping at epoch\", epochs)\n",
    "            print(\"###### Epoch\", epochs, \"######\")\n",
    "            print(\"Error:\", error_hist[epochs], \"%\")\n",
    "            print(\"Thetas:\\n\", theta)\n",
    "            break\n",
    "    \n",
    "    #############################################\n",
    "    # Part 3: Data Plotting and Training Result #\n",
    "    #############################################\n",
    "    # First Figure: Dataset plotting\n",
    "    plt.figure(2)\n",
    "    \n",
    "    plt.title(\"Simplified Iris Dataset Classification\\n(Green=Iris-setosa; Blue=Iris-virginica)\")\n",
    "    plt.xlabel(\"Sepal width (cm)\")\n",
    "    plt.ylabel(\"Sepal length (cm)\")\n",
    "    \n",
    "    pos = np.where(y == 1)\n",
    "    neg = np.where(y == 0)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.plot(x[1,pos], x[2,pos], 'go', x[1,neg], x[2,neg], 'bo')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # Second Figure: Training results\n",
    "    plt.figure(3)\n",
    "    deciBound = decisionBound(theta, x)\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    \n",
    "    plt.title(\"Decision Boundary\\n(Green=Iris-setosa; Blue=Iris-virginica; Black=DecisionBoundary)\")\n",
    "    plt.xlabel(\"Sepal width (cm)\")\n",
    "    plt.ylabel(\"Sepal length (cm)\")\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.plot(x[1,pos], x[2,pos], 'go', x[1,neg], x[2,neg], 'bo', deciBound[0], deciBound[1], 'k-')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    \n",
    "    plt.title(\"Error History\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Least Square Error\")\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.plot(error_hist[:epochs], \"g-\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "#__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
