{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Polinomial\n",
    "\n",
    "### Descrição:\n",
    "Nesse notebook iremos aperfeiçoar o modelo de Regressão Linear que tínhamos, fazendo apenas algumas simples modificações. Uma Regressão Polinomial é um \"nickname\" para uma Regressão cujo modelo resultante será um polinômio de grau n. Essa técnica nos permite realizar uma Regressão mais complexa, no sentido de que nossos modelos podem representar mais do que relações lineares com relação aos dados estudados.\n",
    "\n",
    "Apesar do nome, a Regressão Polinomial ainda é, essencialmente, uma Regressão Linear (pois esse termo se refere aos parâmetros, e não à relação entre as Variáveis). O algoritmo é, no fundo, o mesmo: apenas iremos fazer um tratamento especial nos dados antes de aplicarmos o Gradiente Descendente. Esse tratamento consiste em realizar transformações não-lineares nos dados e adicionar essa transformação como um novo atributo.\n",
    "\n",
    "<b> Obs.: </b> todas as matrizes/vetores utilizados na fundamentação teórica são consideradas como Vetores-Colunas. A implementação pode diferir um pouco dessa convenção.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "%matplotlib qt5\n",
    "\n",
    "# Libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Linear\n",
    "\n",
    "No novo modelo, teremos novos atributos (que serão os atributos transformados) e, para cada um, novos parâmetros $\\theta$. Por fim, nosso modelo poderá ter uma quantidade qualquer de atributos:\n",
    "\n",
    "$$ h(\\theta) = \\theta_{0}+\\theta_{1}X_{1}+\\theta_{1}X_{2}+\\cdots+\\theta_{n}X_{n} $$\n",
    "\n",
    "Apesar dessa mudança de representação, nossa operação matricial ainda é válida e, portante, a função utilizada anteriormente funcionará para o caso geral de qualquer um destes modelos.\n",
    "\n",
    "$$ h(\\theta) = \\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1} \\\\ \\vdots \\\\ \\theta_{n} \\end{bmatrix}^{T} \\times \\begin{bmatrix} 1 \\\\ X_{1} \\\\ \\vdots \\\\ X_{n} \\end{bmatrix} = \\begin{bmatrix} \\theta_{0} & \\theta_{1} & \\cdots & \\theta_{n} \\end{bmatrix} \\times \\begin{bmatrix} 1 \\\\ X_{1} \\\\ \\vdots \\\\ X_{n} \\end{bmatrix}  = \\theta^{T}X $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definição da Função para o Modelo Linear\n",
    "def h_theta(x, theta):\n",
    "    ''' Apply the Linear Model for features X and parameters theta '''\n",
    "    return np.dot(np.transpose(theta), x)\n",
    "\n",
    "# Teste da Função: h([5;2]) = 5+2X+0.5X²\n",
    "testX = np.array([[1,1,1],\n",
    "                  [3,4,9],\n",
    "                  [9,16,81]])\n",
    "\n",
    "testTheta = np.array([[5],\n",
    "                      [2],\n",
    "                      [0.5]])\n",
    "\n",
    "print(\"Prediction:\", h_theta(testX, testTheta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de Custo\n",
    "\n",
    "Na célula abaixo, definimos uma função que calcula e retorna o custo total das nossas predições, isto é, o cálculo do erro total do treinamento. A função que utilizaremos pode ser arbitrária (apenas nos permite ter uma melhor visualização do treinamento, mas não influencia o mesmo).\n",
    "\n",
    "Como estudamos, também, aplicaremos Regularização para melhor adequar a complexidade dos dados.\n",
    "Nesse algoritmo, utilizaremos a Soma dos Resíduos Quadráticos Regularizada:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum (h(\\theta) - y)^{2} = \\frac{1}{m} \\sum ( Erro )^{2} + \\frac{1}{m}\\sum (\\theta)^{2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custo Total: 14.124\n"
     ]
    }
   ],
   "source": [
    "# Definição da Função de Erro\n",
    "def costFunction(errors, theta):\n",
    "    ''' Calculate the Least Square Error '''\n",
    "    return (1 / np.size(errors)) * (np.sum(errors ** 2) + np.sum(theta[1:] ** 2))\n",
    "\n",
    "# Teste da Função\n",
    "errors = np.array([5., 4., 4., 3., 2.])\n",
    "theta_error = np.array([[1000],\n",
    "                        [0.2],\n",
    "                        [0.7],\n",
    "                        [0.3]])\n",
    "\n",
    "print(\"Custo Total:\", costFunction(errors, theta_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração de Features\n",
    "\n",
    "Na célula abaixo, definimos a função que será responsável por extrair o conjunto de <i>features</i> de todo o Dataset. Utilizaremos essa função pois, com ela, podemos aumentar a complexidade do nosso modelo ao criar novos atributos que sejam transformações não-lineares dos <i>features</i> do Dataset.\n",
    "\n",
    "É importante notar que caso o Dataset já possua $n$ <i>features</i>, cada transformação será aplicada às $n$ <i>features</i> e incluida no conjunto, implicando num crescimento de atributos igual a $(Kn)$, onde $K$ representa o grau de complexidade do polinômio dessa transformação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definição da Função de Extração\n",
    "def featureExtraction(data, n_examples, complexity=1):\n",
    "    ''' Extracts the features from a dataset and apply polynomial transformations '''\n",
    "    x = np.ones(n_examples)\n",
    "    \n",
    "    for i in range(0, complexity):\n",
    "        x = np.vstack([x, np.transpose(data[:, 0:-1]) ** (i+1)])\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Teste da Função\n",
    "dataExtract = np.array([[2, 3, 0],\n",
    "                        [5, 6, 0],\n",
    "                        [8, 9, 0]])\n",
    "\n",
    "x_extract = featureExtraction(dataExtract, 3, 1)\n",
    "print(\"Features with complexity 1:\\n\", x_extract)\n",
    "print(\"\")\n",
    "\n",
    "x_extract = featureExtraction(dataExtract, 3, 3)\n",
    "print(\"Features with complexity 3:\\n\", x_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Na célula abaixo criamos uma função para <b>normalizar</b> as nossas <i>features</i>. O objetivo, como mostrado, é evitar problemas de divergência e permitir um melhor treinamento por meio dos hiperparâmetros.\n",
    "\n",
    "A fórmula que usaremos para o <i>Feature Scaling</i> será:\n",
    "\n",
    "$$\n",
    "    X = \\frac{X - \\bar{X}}{\\sigma(X)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definição da Função de Normalização\n",
    "def normalizeData(data):\n",
    "    ''' Apply Feature Scaling to the features set '''\n",
    "    for i in range(1, np.size(data,0)):\n",
    "        data[i,:] = (data[i,:]-np.mean(data[i,:])) / np.std(data[i,:])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Teste da Função\n",
    "x_norm = np.array([[1., 1., 1., 1., 1.],\n",
    "                   [2., 9., 4., 7., 5.],\n",
    "                   [-5.,-4.,-2.,-9.,-8.]])\n",
    "\n",
    "x_norm = normalizeData(x_norm)\n",
    "print(\"Features normalizados:\\n\",x_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programa Principal (Regressão Linear)\n",
    "\n",
    "No programa principal, iremos programar a Regressão Linear propriamente dita.\n",
    "Dividimos o código em três partes:\n",
    "\n",
    "### Part 1: Data Pre-Processing\n",
    "\n",
    "Nesse trecho, iremos nos preocupar em carregar e organizar o dataset que utilizaremos no treinamento. É nesse momento, também, que iremos declarar e separar as variáveis que definem o Conjunto de Atributos, o Conjunto de Saída e os Parâmetros do Modelo, além dos Hiperparâmetros de Treinamento. Iremos seguir a convenção de considerar todos os exemplares como vetores-colunas. No entanto, o numpy não nos permite facilmente modificar essa informação para o Conjunto de Saída, e o mesmo continuará como vetor-linha (sem muito prejuízo). Iremos criar, também um vetor para armazenar o Histórico de Erros do treinamento (por motivos de visualização).\n",
    "\n",
    "Iremos utilizar o dataset <i>data1.txt</i> localizado na pasta <i>datasets/</i>. Teremos as seguintes matrizes:\n",
    "\n",
    "$$\n",
    "    X = \\begin{bmatrix} 1 & 1 & \\cdots & 1 \\\\  X_{1}^{(1)} & X_{1}^{(2)} & \\cdots & X_{1}^{(m)}  \\end{bmatrix};\\   \\theta = \\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1}\\end{bmatrix};\\  Y = \\begin{bmatrix} Y^{(1)} & Y^{(2)} & \\cdots & Y^{(m)} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Part 2: Linear Regression Training\n",
    "\n",
    "Para cada época até a convergência (ou até atingir o limite máximo definido pelo Hiperparâmetro) iremos realizar o Treinamento da Regressão Linear. Os passos serão os seguintes:\n",
    "\n",
    "1. Calculamos o vetor de predição \"Y_pred\", como resultado da predição do Modelo para os parâmetros daquela época;\n",
    "2. Utilizando \"Y_pred\", calculamos os erros de acordo com o a matriz real \"Y\";\n",
    "3. Concatenamos o Custo Total do erro calculado no Histórico de Erros;\n",
    "4. Realizamos, para cada parâmetro, o Gradiente Descendente para estimar os novos valores dos parâmetros;\n",
    "5. Imprimimos os resultados do treino a cada 500 épocas;\n",
    "6. Verificamos uma possível convergência do treino, e paremos o mesmo caso seja verificado;\n",
    "\n",
    "### Part 3: Data Plotting and Training Results\n",
    "\n",
    "Ao fim do treinamento, iremos plotar duas figuras para avaliar o resultado final do nosso algoritmo. A <b>Figura 1</b> irá apenas exibir os atributos do Dataset. A <b>Figura 2</b> irá exibir a função estimada pelo nosso Modelo Linear, além do Histórico de Erros dado as épocas até convergência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Epoch 0 ######\n",
      "Error: 405.163274763\n",
      "Thetas:\n",
      " [[-0.20715822]\n",
      " [ 0.17538592]\n",
      " [-0.03240828]\n",
      " [ 0.01087133]\n",
      " [-0.04442315]\n",
      " [-0.04126007]\n",
      " [-0.05634375]\n",
      " [-0.05836599]\n",
      " [-0.06322424]\n",
      " [-0.06478691]\n",
      " [-0.06649269]\n",
      " [-0.06720115]\n",
      " [-0.06771617]\n",
      " [-0.06786458]\n",
      " [-0.06786154]\n",
      " [-0.06770491]\n",
      " [-0.06746504]\n",
      " [-0.06715993]\n",
      " [-0.06681801]\n",
      " [-0.06645304]\n",
      " [-0.0660783 ]\n",
      " [-0.06570196]\n",
      " [-0.06533057]\n",
      " [-0.06496847]\n",
      " [-0.06461879]\n",
      " [-0.06428354]\n",
      " [-0.06396401]\n",
      " [-0.06366088]\n",
      " [-0.0633744 ]\n",
      " [-0.06310452]\n",
      " [-0.06285093]\n",
      " [-0.06261318]\n",
      " [-0.06239071]\n",
      " [-0.06218285]\n",
      " [-0.06198891]\n",
      " [-0.06180818]\n",
      " [-0.06163993]\n",
      " [-0.06148343]\n",
      " [-0.06133799]\n",
      " [-0.0612029 ]\n",
      " [-0.06107751]\n",
      " [-0.06096119]\n",
      " [-0.06085333]\n",
      " [-0.06075335]\n",
      " [-0.06066073]\n",
      " [-0.06057493]\n",
      " [-0.06049549]\n",
      " [-0.06042196]\n",
      " [-0.0603539 ]\n",
      " [-0.06029094]\n",
      " [-0.06023269]]\n",
      "\n",
      "###### Epoch 500 ######\n",
      "Error: 42.5746929752\n",
      "Thetas:\n",
      " [[ -1.03574946e+01]\n",
      " [  1.88613078e+01]\n",
      " [ -8.38723624e+00]\n",
      " [  1.79302068e+00]\n",
      " [ -3.62691087e+00]\n",
      " [ -1.18733849e+00]\n",
      " [ -1.98567515e+00]\n",
      " [ -1.25617922e+00]\n",
      " [ -1.22852809e+00]\n",
      " [ -9.19844339e-01]\n",
      " [ -7.83291137e-01]\n",
      " [ -6.10167478e-01]\n",
      " [ -4.91188960e-01]\n",
      " [ -3.79480650e-01]\n",
      " [ -2.91547020e-01]\n",
      " [ -2.15403142e-01]\n",
      " [ -1.52812821e-01]\n",
      " [ -9.98259217e-02]\n",
      " [ -5.56509754e-02]\n",
      " [ -1.84868849e-02]\n",
      " [  1.26340573e-02]\n",
      " [  3.87605783e-02]\n",
      " [  6.06551092e-02]\n",
      " [  7.90072130e-02]\n",
      " [  9.43709630e-02]\n",
      " [  1.07222829e-01]\n",
      " [  1.17958568e-01]\n",
      " [  1.26913928e-01]\n",
      " [  1.34370915e-01]\n",
      " [  1.40567964e-01]\n",
      " [  1.45706087e-01]\n",
      " [  1.49955066e-01]\n",
      " [  1.53458164e-01]\n",
      " [  1.56336294e-01]\n",
      " [  1.58691445e-01]\n",
      " [  1.60609610e-01]\n",
      " [  1.62163247e-01]\n",
      " [  1.63413365e-01]\n",
      " [  1.64411292e-01]\n",
      " [  1.65200170e-01]\n",
      " [  1.65816228e-01]\n",
      " [  1.66289848e-01]\n",
      " [  1.66646484e-01]\n",
      " [  1.66907428e-01]\n",
      " [  1.67090465e-01]\n",
      " [  1.67210427e-01]\n",
      " [  1.67279655e-01]\n",
      " [  1.67308401e-01]\n",
      " [  1.67305161e-01]\n",
      " [  1.67276953e-01]\n",
      " [  1.67229561e-01]]\n",
      "\n",
      "###### Epoch 1000 ######\n",
      "Error: 36.5587575163\n",
      "Thetas:\n",
      " [[ -1.03579110e+01]\n",
      " [  1.95827465e+01]\n",
      " [ -9.78472602e+00]\n",
      " [  1.82388631e+00]\n",
      " [ -3.86437383e+00]\n",
      " [ -1.06866397e+00]\n",
      " [ -1.88845375e+00]\n",
      " [ -1.07690761e+00]\n",
      " [ -1.05554333e+00]\n",
      " [ -7.37860529e-01]\n",
      " [ -6.13893265e-01]\n",
      " [ -4.51517777e-01]\n",
      " [ -3.48186770e-01]\n",
      " [ -2.51468255e-01]\n",
      " [ -1.78964224e-01]\n",
      " [ -1.17343473e-01]\n",
      " [ -6.85405970e-02]\n",
      " [ -2.83164840e-02]\n",
      " [  4.05206563e-03]\n",
      " [  3.03782371e-02]\n",
      " [  5.15581867e-02]\n",
      " [  6.85847036e-02]\n",
      " [  8.21483228e-02]\n",
      " [  9.28736588e-02]\n",
      " [  1.01250817e-01]\n",
      " [  1.07698337e-01]\n",
      " [  1.12558668e-01]\n",
      " [  1.16119629e-01]\n",
      " [  1.18619995e-01]\n",
      " [  1.20259427e-01]\n",
      " [  1.21204095e-01]\n",
      " [  1.21592474e-01]\n",
      " [  1.21539623e-01]\n",
      " [  1.21141005e-01]\n",
      " [  1.20475582e-01]\n",
      " [  1.19608461e-01]\n",
      " [  1.18593114e-01]\n",
      " [  1.17473250e-01]\n",
      " [  1.16284405e-01]\n",
      " [  1.15055277e-01]\n",
      " [  1.13808860e-01]\n",
      " [  1.12563399e-01]\n",
      " [  1.11333197e-01]\n",
      " [  1.10129296e-01]\n",
      " [  1.08960050e-01]\n",
      " [  1.07831603e-01]\n",
      " [  1.06748295e-01]\n",
      " [  1.05713002e-01]\n",
      " [  1.04727416e-01]\n",
      " [  1.03792280e-01]\n",
      " [  1.02907588e-01]]\n",
      "\n",
      "###### Epoch 1500 ######\n",
      "Error: 35.8875845833\n",
      "Thetas:\n",
      " [[ -1.03579110e+01]\n",
      " [  1.96477009e+01]\n",
      " [ -1.00046732e+01]\n",
      " [  1.85770069e+00]\n",
      " [ -3.86168224e+00]\n",
      " [ -1.01756878e+00]\n",
      " [ -1.84917548e+00]\n",
      " [ -1.03603380e+00]\n",
      " [ -1.02496910e+00]\n",
      " [ -7.13977086e-01]\n",
      " [ -5.98089639e-01]\n",
      " [ -4.41929434e-01]\n",
      " [ -3.44092943e-01]\n",
      " [ -2.51579669e-01]\n",
      " [ -1.82370772e-01]\n",
      " [ -1.23143363e-01]\n",
      " [ -7.60314559e-02]\n",
      " [ -3.69009735e-02]\n",
      " [ -5.15923632e-03]\n",
      " [  2.09113063e-02]\n",
      " [  4.21189268e-02]\n",
      " [  5.93874854e-02]\n",
      " [  7.33500039e-02]\n",
      " [  8.45857736e-02]\n",
      " [  9.35487079e-02]\n",
      " [  1.00629201e-01]\n",
      " [  1.06147885e-01]\n",
      " [  1.10375980e-01]\n",
      " [  1.13539786e-01]\n",
      " [  1.15829786e-01]\n",
      " [  1.17405551e-01]\n",
      " [  1.18400985e-01]\n",
      " [  1.18928142e-01]\n",
      " [  1.19080686e-01]\n",
      " [  1.18936696e-01]\n",
      " [  1.18561088e-01]\n",
      " [  1.18007651e-01]\n",
      " [  1.17320783e-01]\n",
      " [  1.16536972e-01]\n",
      " [  1.15686047e-01]\n",
      " [  1.14792244e-01]\n",
      " [  1.13875119e-01]\n",
      " [  1.12950311e-01]\n",
      " [  1.12030198e-01]\n",
      " [  1.11124448e-01]\n",
      " [  1.10240487e-01]\n",
      " [  1.09383891e-01]\n",
      " [  1.08558715e-01]\n",
      " [  1.07767779e-01]\n",
      " [  1.07012893e-01]\n",
      " [  1.06295057e-01]]\n",
      "\n",
      "###### Epoch 2000 ######\n",
      "Error: 35.7945317636\n",
      "Thetas:\n",
      " [[ -1.03579110e+01]\n",
      " [  1.96522580e+01]\n",
      " [ -1.00481602e+01]\n",
      " [  1.86971760e+00]\n",
      " [ -3.85252273e+00]\n",
      " [ -1.00098536e+00]\n",
      " [ -1.83646517e+00]\n",
      " [ -1.02559471e+00]\n",
      " [ -1.01849244e+00]\n",
      " [ -7.10578063e-01]\n",
      " [ -5.97549723e-01]\n",
      " [ -4.43555854e-01]\n",
      " [ -3.47407879e-01]\n",
      " [ -2.56077337e-01]\n",
      " [ -1.87660061e-01]\n",
      " [ -1.28887049e-01]\n",
      " [ -8.19683954e-02]\n",
      " [ -4.28244131e-02]\n",
      " [ -1.09138911e-02]\n",
      " [  1.54407320e-02]\n",
      " [  3.70141625e-02]\n",
      " [  5.47040141e-02]\n",
      " [  6.91224326e-02]\n",
      " [  8.08326464e-02]\n",
      " [  9.02762621e-02]\n",
      " [  9.78344693e-02]\n",
      " [  1.03821133e-01]\n",
      " [  1.08502643e-01]\n",
      " [  1.12101978e-01]\n",
      " [  1.14807470e-01]\n",
      " [  1.16777433e-01]\n",
      " [  1.18145194e-01]\n",
      " [  1.19022735e-01]\n",
      " [  1.19504023e-01]\n",
      " [  1.19667702e-01]\n",
      " [  1.19579442e-01]\n",
      " [  1.19293909e-01]\n",
      " [  1.18856454e-01]\n",
      " [  1.18304558e-01]\n",
      " [  1.17669054e-01]\n",
      " [  1.16975177e-01]\n",
      " [  1.16243459e-01]\n",
      " [  1.15490487e-01]\n",
      " [  1.14729546e-01]\n",
      " [  1.13971170e-01]\n",
      " [  1.13223606e-01]\n",
      " [  1.12493204e-01]\n",
      " [  1.11784751e-01]\n",
      " [  1.11101749e-01]\n",
      " [  1.10446649e-01]\n",
      " [  1.09821049e-01]]\n",
      "\n",
      "Gradient Converged!!!\n",
      "Stopping at epoch 2389\n",
      "###### Epoch 2389 ######\n",
      "Error: 35.7802069904\n",
      "Thetas:\n",
      " [[ -1.03579110e+01]\n",
      " [  1.96521505e+01]\n",
      " [ -1.00573281e+01]\n",
      " [  1.87274376e+00]\n",
      " [ -3.84924955e+00]\n",
      " [ -9.96604100e-01]\n",
      " [ -1.83302707e+00]\n",
      " [ -1.02306155e+00]\n",
      " [ -1.01705715e+00]\n",
      " [ -7.10059123e-01]\n",
      " [ -5.97811335e-01]\n",
      " [ -4.44409632e-01]\n",
      " [ -3.48699585e-01]\n",
      " [ -2.57664602e-01]\n",
      " [ -1.89428869e-01]\n",
      " [ -1.30743798e-01]\n",
      " [ -8.38406998e-02]\n",
      " [ -4.46566555e-02]\n",
      " [ -1.26650473e-02]\n",
      " [  1.38001076e-02]\n",
      " [  3.55041196e-02]\n",
      " [  5.33372753e-02]\n",
      " [  6.79060173e-02]\n",
      " [  7.97692422e-02]\n",
      " [  8.93653105e-02]\n",
      " [  9.70730390e-02]\n",
      " [  1.03204603e-01]\n",
      " [  1.08025239e-01]\n",
      " [  1.11757182e-01]\n",
      " [  1.14588332e-01]\n",
      " [  1.16676813e-01]\n",
      " [  1.18155931e-01]\n",
      " [  1.19137777e-01]\n",
      " [  1.19716515e-01]\n",
      " [  1.19971050e-01]\n",
      " [  1.19967353e-01]\n",
      " [  1.19760413e-01]\n",
      " [  1.19395918e-01]\n",
      " [  1.18911684e-01]\n",
      " [  1.18338879e-01]\n",
      " [  1.17703064e-01]\n",
      " [  1.17025083e-01]\n",
      " [  1.16321822e-01]\n",
      " [  1.15606851e-01]\n",
      " [  1.14890971e-01]\n",
      " [  1.14182682e-01]\n",
      " [  1.13488569e-01]\n",
      " [  1.12813640e-01]\n",
      " [  1.12161604e-01]\n",
      " [  1.11535102e-01]\n",
      " [  1.10935913e-01]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Main Function\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    ###############################\n",
    "    # Part 1: Data Pre-Processing #\n",
    "    ###############################\n",
    "    # Loads the data\n",
    "    data = np.loadtxt(\"../datasets/dataPoly1.txt\")\n",
    "    \n",
    "    n_examples = np.size(data, 0)\n",
    "    \n",
    "    # Define the model parameters\n",
    "    x = normalizeData(featureExtraction(data, n_examples, 50))    \n",
    "    y = data[:, 1]\n",
    "    \n",
    "    n_features = np.size(x, 0)\n",
    "    \n",
    "    theta = np.zeros([n_features, 1])\n",
    "\n",
    "    # Defines the hyperparameters and training measurements\n",
    "    alfa = 0.02\n",
    "    gamma = 2\n",
    "    max_epochs = 50000\n",
    "    \n",
    "    error_hist = np.zeros([max_epochs])\n",
    "    epsilon = 0.001\n",
    "    \n",
    "    ######################################\n",
    "    # Part 2: Linear Regression Training #\n",
    "    ######################################\n",
    "    for epochs in range(max_epochs):\n",
    "        # Calculate the error vector from the current Model\n",
    "        y_pred = h_theta(x, theta)\n",
    "        error = y_pred - y\n",
    "\n",
    "        # Append new Least Square Error to History\n",
    "        error_hist[epochs] = errorFunction(error)\n",
    "\n",
    "        # Perform Gradient Descent\n",
    "        for j in range(n_features):\n",
    "            if (j >= 1):\n",
    "                theta[j] = theta[j] - (alfa/n_examples) * (np.sum(error * x[j,:]) + (gamma * theta[j]))\n",
    "            else:\n",
    "                theta[j] = theta[j] - (alfa/n_examples) * np.sum(error * x[j,:])\n",
    "\n",
    "        # Prints training status at each 100 epochs\n",
    "        if(epochs % 500 == 0):\n",
    "            print(\"###### Epoch\", epochs, \"######\")\n",
    "            print(\"Error:\", error_hist[epochs])\n",
    "            print(\"Thetas:\\n\", theta)\n",
    "            print(\"\")\n",
    "        \n",
    "        # Evaluate convergence and stops training if so\n",
    "        if(abs(error_hist[epochs] - error_hist[epochs-50]) <= epsilon):\n",
    "            print(\"Gradient Converged!!!\\nStopping at epoch\", epochs)\n",
    "            print(\"###### Epoch\", epochs, \"######\")\n",
    "            print(\"Error:\", error_hist[epochs])\n",
    "            print(\"Thetas:\\n\", theta)\n",
    "            print(\"\")\n",
    "            break\n",
    "            \n",
    "    #############################################\n",
    "    # Part 3: Data Plotting and Training Result #\n",
    "    #############################################\n",
    "    # First Figure: Dataset plotting\n",
    "    plt.figure(1)\n",
    "    \n",
    "    plt.title(\"Artificial Generated Data with Noise\\n $f(x)=-13.15648 + 1.4928 * X$\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"f(X)\")\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.plot(x[1,:], y, 'rx')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Second Figure: Training results\n",
    "    plt.figure(2)\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Artificial Generated Data with Noise\\n $f(x)=-13.15648 + 1.4928 * X$\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"f(X)\")\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.plot(x[1,:], y, 'rx', x[1,:], h_theta(x, theta)[0,:], 'k-')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(\"Error History\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Least Square Error\")\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.plot(error_hist[:epochs], \"g-\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "#__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
