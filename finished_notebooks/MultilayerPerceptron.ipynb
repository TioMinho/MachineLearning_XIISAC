{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singlelayer Perceptron\n",
    "\n",
    "### Descrição:\n",
    "Nesse notebook iremos utilizar a arquitetura do <b>Singlelayer Perceptron</b> para realizar uma <i>multiclass classification</i> para o famoso problema do Iris-Dataset.\n",
    "\n",
    "O Singlelayer Perceptron, uma Rede Neural Artificial, possui exatamente os mesmos algoritmos de uma Regressão Logística. Para exemplificarmos seu potencial, desenvolvemos um problema de classificação multiclasse para expressar seu funcionamento e motivar a preparação para o Multilayer Perceptron. No problema de <i>multiclass classification</i>, devemos sempre nos preocupar em enumerar nossas classes e aplicar o <i>one-hot encoding</i>.\n",
    "\n",
    "<b> Obs.: </b> todas as matrizes/vetores utilizados na fundamentação teórica são consideradas como Vetores-Colunas. A implementação pode diferir um pouco dessa convenção.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "%matplotlib qt5\n",
    "\n",
    "# Libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import random as rnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Perceptron\n",
    "\n",
    "O Singlelayer Perceptron difere da Regressão Logística (em sua convenção) por utilizar uma diferente \"função logística\",  a <b>Função Sinal</b>, que possui a seguinte regra:\n",
    "\n",
    "$$\n",
    "    \\varphi(x) = \\begin{cases} 1 & \\text{ if } x \\geq 0 \\\\  0 & \\text{ if } x < 0 \\end{cases}\n",
    "$$\n",
    "\n",
    "No entanto, essa função possui algumas dificuldades de implementação, além de ser vulnerável ao problema de dupla-classificação. Logo, continuaremos utilizando a função logística Sigmoide como função ativadora.\n",
    "\n",
    "$$\n",
    "    \\varphi(x) = \\frac{1}{1+e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definição das Funções Sigmoide e Modelo Perceptron\n",
    "def sigmoid(X):\n",
    "    ''' Returns the Sigmoid Function applied to a vector (or value) x '''\n",
    "    return 1 / (1 + np.exp(-1 * X))\n",
    "    \n",
    "def h_theta(X, theta):\n",
    "    ''' Apply the Linear Model for features X and parameters theta '''\n",
    "    return sigmoid(np.dot(np.transpose(theta), X))\n",
    "\n",
    "# Teste da Sigmoide\n",
    "testSigmoid = np.linspace(-30, 30)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title(\"Sigmoid Function\")\n",
    "plt.plot(testSigmoid, sigmoid(testSigmoid), 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "Os Datasets não trarão as classes já configuradas em One-Hot Encoding. Ao invés disso, encontraremos muitos datasets que as classes serão enumeradas (de $1..k$). Esse é o caso do Iris-Dataset.\n",
    "\n",
    "Criaremos, então, uma função que receba tal enumeração e a transforme em uma matriz One-Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "def oneHotEncoding(y):\n",
    "    y = y.astype(int)\n",
    "    \n",
    "    classes = np.max(y)\n",
    "    examples = np.size(y,0)\n",
    "    oneHot = np.zeros([classes, examples])\n",
    "    \n",
    "    for j in range(examples):\n",
    "        oneHot[y[j]-1, j] = 1\n",
    "        \n",
    "    return oneHot\n",
    "        \n",
    "# Teste\n",
    "y = np.array([1, 3, 4, 5 ,1, 1, 2, 3, 4, 2, 5])\n",
    "\n",
    "y = oneHotEncoding(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de Predição\n",
    "\n",
    "No nosso modelo, nossas predições estarão no formato <i>one-hot encoding</i>. No entanto, como estamos usando a Sigmoide, nossas predições terão valores dentro do intervalo [0,1]. Para realizar a classificação corretamente, iremos escrever uma função que irá verifiar, para cada exemplar, qual classe ele terá mais probabilidade de pertencer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Prediction:\n",
      " [[ 0.93702664  0.0099518   0.21416502]\n",
      " [ 0.5         0.450166    0.80218389]\n",
      " [ 0.7109495   0.11920292  0.52497919]]\n",
      "\n",
      "After Prediction:\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  1.  1.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Definição da Função de OneHotEncoding\n",
    "def predictOHE(y):\n",
    "    classes  = np.size(y,0)\n",
    "    examples = np.size(y,1)\n",
    "    \n",
    "    oneHot = np.zeros([classes, examples])\n",
    "    \n",
    "    for j in range(examples):\n",
    "        maxPos = np.argmax(y[:,j], axis=0)\n",
    "        oneHot[maxPos,j] = 1\n",
    "    \n",
    "    return oneHot\n",
    "    \n",
    "# Teste da Função de OneHotEncoding\n",
    "x_ohe = np.array([[-10, 10, 7],\n",
    "                  [  5, -8,-3],\n",
    "                  [  2,  0, 1]])\n",
    "\n",
    "theta_ohe = np.array([[0.1, 0.3, 0.2],\n",
    "                      [0.7, 0.4, 0.5],\n",
    "                      [0.1, 0.5, 0.2]])\n",
    "\n",
    "y_pred_ohe = h_theta(x_ohe, theta_ohe)\n",
    "print(\"Before Prediction:\\n\", y_pred_ohe)\n",
    "print(\"\")\n",
    "\n",
    "y_pred_ohe = predictOHE(y_pred_ohe)\n",
    "print(\"After Prediction:\\n\", y_pred_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de Acurácia\n",
    "\n",
    "Em problemas de Classificação, subtrair o valor previsto do valor real não nos dá uma informação muito precisa (sobre, por exemplo, a gravidade do erro). Para avaliar melhor a qualidade de um Treinamento de Classificação existem diversas medidas. Uma bem comum, e simples, consiste na acurácia: a quantidade de exemplos \"corretamente classificados\". Vamos definir essa porcentagem como sendo:\n",
    "\n",
    "$$ Acc(\\theta) = 100 \\times (1 - \\frac{1}{m} \\sum (h(\\theta) - y)^{2}) $$\n",
    "\n",
    "<b>Obs.:</b> a acurácia só será correta caso tanto $h(\\theta)$ quanto $y$ sejam valores binários. Logo, iremos arredondar os resultados de $h(\\theta)$ utilizando a função <b>Predict()</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracyFunction(X, y, theta):\n",
    "    ''' Calculates the percentage of correct classifications '''\n",
    "    Y_pred = predictOHE(h_theta(X, theta))\n",
    "    \n",
    "    return 100 * (1 - (1 / np.size(y)) * np.sum((Y_pred - y) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração de Features\n",
    "\n",
    "Na célula abaixo, definimos a função que será responsável por extrair o conjunto de <i>features</i> de todo o Dataset. Utilizaremos essa função pois, com ela, podemos aumentar a complexidade do nosso modelo ao criar novos atributos que sejam transformações não-lineares dos <i>features</i> do Dataset.\n",
    "\n",
    "É importante notar que caso o Dataset já possua $n$ <i>features</i>, cada transformação será aplicada às $n$ <i>features</i> e incluida no conjunto, implicando num crescimento de atributos igual a $(Kn)$, onde $K$ representa o grau de complexidade do polinômio dessa transformação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with complexity 1:\n",
      " [[ 1.  1.  1.]\n",
      " [ 2.  5.  8.]\n",
      " [ 3.  6.  9.]]\n",
      "\n",
      "Features with complexity 3:\n",
      " [[   1.    1.    1.]\n",
      " [   2.    5.    8.]\n",
      " [   3.    6.    9.]\n",
      " [   4.   25.   64.]\n",
      " [   9.   36.   81.]\n",
      " [   8.  125.  512.]\n",
      " [  27.  216.  729.]]\n"
     ]
    }
   ],
   "source": [
    "# Definição da Função de Extração\n",
    "def featureExtraction(data, n_examples, complexity=1):\n",
    "    ''' Extracts the features from a dataset and apply polynomial transformations '''\n",
    "    x = np.ones(n_examples)\n",
    "    \n",
    "    for i in range(0, complexity):\n",
    "        x = np.vstack([x, np.transpose(data[:, 0:-1]) ** (i+1)])\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Teste da Função\n",
    "dataExtract = np.array([[2, 3, 0],\n",
    "                        [5, 6, 0],\n",
    "                        [8, 9, 0]])\n",
    "\n",
    "x_extract = featureExtraction(dataExtract, 3, 1)\n",
    "print(\"Features with complexity 1:\\n\", x_extract)\n",
    "print(\"\")\n",
    "\n",
    "x_extract = featureExtraction(dataExtract, 3, 3)\n",
    "print(\"Features with complexity 3:\\n\", x_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Na célula abaixo criamos uma função para <b>normalizar</b> as nossas <i>features</i>. O objetivo, como mostrado, é evitar problemas de divergência e permitir um melhor treinamento por meio dos hiperparâmetros.\n",
    "\n",
    "A fórmula que usaremos para o <i>Feature Scaling</i> será:\n",
    "\n",
    "$$\n",
    "    X = \\frac{X - \\bar{X}}{\\sigma(X)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features normalizados:\n",
      " [[ 1.          1.          1.          1.          1.        ]\n",
      " [-1.40693001  1.4896906  -0.57932412  0.66208471 -0.16552118]\n",
      " [ 0.23284516  0.62092042  1.39707095 -1.31945589 -0.93138063]]\n"
     ]
    }
   ],
   "source": [
    "# Definição da Função de Normalização\n",
    "def normalizeData(data):\n",
    "    ''' Apply Feature Scaling to the features set '''\n",
    "    for i in range(1, np.size(data,0)):\n",
    "        data[i,:] = (data[i,:]-np.mean(data[i,:])) / np.std(data[i,:])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Teste da Função\n",
    "x_norm = np.array([[1., 1., 1., 1., 1.],\n",
    "                   [2., 9., 4., 7., 5.],\n",
    "                   [-5.,-4.,-2.,-9.,-8.]])\n",
    "\n",
    "x_norm = normalizeData(x_norm)\n",
    "print(\"Features normalizados:\\n\",x_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linha de Decisão\n",
    "\n",
    "Na célula abaixo iremos definir uma função simples para calcular e retornar os eixos cartesianos da Linha de Decisão estimada pelo modelo. Essa função apenas utiliza os parâmetros $\\theta$ para calcular o coeficiente angular e linear da reta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decisionBound(theta, x):\n",
    "    ''' Calculates and returns a linear Decision Boundary from the model '''\n",
    "    boundary_X = np.linspace(min(x[1,:]), max(x[1,:]))\n",
    "    boundary_Y = -1*(theta[1] / theta[2]) * boundary_X - (theta[0] / theta[2]);\n",
    "    return [boundary_X, boundary_Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5  0.5 ...,  0.5  0.5  0.5]\n",
      " [ 0.5  0.5  0.5 ...,  0.5  0.5  0.5]\n",
      " [ 0.5  0.5  0.5 ...,  0.5  0.5  0.5]\n",
      " ..., \n",
      " [ 0.5  0.5  0.5 ...,  0.5  0.5  0.5]\n",
      " [ 0.5  0.5  0.5 ...,  0.5  0.5  0.5]\n",
      " [ 0.5  0.5  0.5 ...,  0.5  0.5  0.5]]\n",
      "10 150\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-c05d81154bf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mfeedFoward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTheta1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTheta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-c05d81154bf8>\u001b[0m in \u001b[0;36mfeedFoward\u001b[0;34m(X, Theta1, Theta2)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_H\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_theta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTheta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/petcomp/anaconda3/lib/python3.5/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "def feedFoward(X, Theta1, Theta2):\n",
    "    \n",
    "\n",
    "    a1 = h_theta(X, Theta1)\n",
    "    n_H = np.size(Theta1, 1)\n",
    "    print(a1)\n",
    "    print(np.size(a1,0), np.size(a1,1))\n",
    "    \n",
    "    H = np.vstack([np.ones(n_H), a1])\n",
    "    a2 = h_theta(H, Theta2)\n",
    "    \n",
    "    return [a1, a2]\n",
    "\n",
    "feedFoward(x,Theta1,Theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backPropagation(a1, a2, y, Theta1, Theta2):\n",
    "    m = np.size(y)\n",
    "    \n",
    "    delta_2 = a2 - y\n",
    "    delta_1 = np.dot(np.transpose(Theta2), delta_2) * (a2) * (1 - a2)\n",
    "                     \n",
    "    grad1 = (1/m) * (delta_1 * a1)\n",
    "    grad1 = grad1[:,1:]\n",
    "    \n",
    "    grad2 = (1/m) * (delta_2 * a2)\n",
    "    \n",
    "    return [grad1, grad2]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programa Principal (Regressão Linear)\n",
    "\n",
    "No programa principal, iremos programar a Regressão Linear propriamente dita.\n",
    "Dividimos o código em três partes:\n",
    "\n",
    "### Part 1: Data Pre-Processing\n",
    "\n",
    "Nesse trecho, iremos nos preocupar em carregar e organizar o dataset que utilizaremos no treinamento. É nesse momento, também, que iremos declarar e separar as variáveis que definem o Conjunto de Atributos, o Conjunto de Saída e os Parâmetros do Modelo, além dos Hiperparâmetros de Treinamento. Iremos seguir a convenção de considerar todos os exemplares como vetores-colunas. No entanto, o numpy não nos permite facilmente modificar essa informação para o Conjunto de Saída, e o mesmo continuará como vetor-linha (sem muito prejuízo). Iremos criar, também um vetor para armazenar o Histórico de Erros do treinamento (por motivos de visualização).\n",
    "\n",
    "Iremos utilizar o dataset <i>data1.txt</i> localizado na pasta <i>datasets/</i>. Teremos as seguintes matrizes:\n",
    "\n",
    "$$\n",
    "    X = \\begin{bmatrix} 1 & 1 & \\cdots & 1 \\\\  X_{1}^{(1)} & X_{1}^{(2)} & \\cdots & X_{1}^{(m)}  \\end{bmatrix};\\   \\theta = \\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1}\\end{bmatrix};\\  Y = \\begin{bmatrix} Y^{(1)} & Y^{(2)} & \\cdots & Y^{(m)} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Part 2: Linear Regression Training\n",
    "\n",
    "Para cada época até a convergência (ou até atingir o limite máximo definido pelo Hiperparâmetro) iremos realizar o Treinamento da Regressão Linear. Os passos serão os seguintes:\n",
    "\n",
    "1. Calculamos o vetor de predição \"Y_pred\", como resultado da predição do Modelo para os parâmetros daquela época;\n",
    "2. Utilizando \"Y_pred\", calculamos os erros de acordo com o a matriz real \"Y\";\n",
    "3. Concatenamos o Custo Total do erro calculado no Histórico de Erros;\n",
    "4. Realizamos, para cada parâmetro, o Gradiente Descendente para estimar os novos valores dos parâmetros;\n",
    "5. Imprimimos os resultados do treino a cada 500 épocas;\n",
    "6. Verificamos uma possível convergência do treino, e paremos o mesmo caso seja verificado;\n",
    "\n",
    "### Part 3: Data Plotting and Training Results\n",
    "\n",
    "Ao fim do treinamento, iremos plotar duas figuras para avaliar o resultado final do nosso algoritmo. A <b>Figura 1</b> irá apenas exibir os atributos do Dataset. A <b>Figura 2</b> irá exibir a função estimada pelo nosso Modelo Linear, além do Histórico de Erros dado as épocas até convergência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-83bbfc7a9eb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeedFoward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTheta1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTheta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-0bab0523ca72>\u001b[0m in \u001b[0;36mfeedFoward\u001b[0;34m(X, Theta1, Theta2)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_theta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTheta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_H\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_theta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTheta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/petcomp/anaconda3/lib/python3.5/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "# Main Function\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    ###############################\n",
    "    # Part 1: Data Pre-Processing #\n",
    "    ###############################\n",
    "    # Loads the data\n",
    "    data = np.loadtxt(\"../datasets/irisDataset.txt\")\n",
    "    \n",
    "    n_examples = np.size(data, 0)\n",
    "\n",
    "    # Defines the hyperparameters and training measurements\n",
    "    alfa = 1\n",
    "    gamma = 0\n",
    "    max_epochs = 500000\n",
    "\n",
    "    neurons_Hidden = 10\n",
    "    \n",
    "    error_hist = np.zeros([max_epochs])\n",
    "    epsilon = 0.001\n",
    "    \n",
    "    # Define the model parameters\n",
    "    x = normalizeData(featureExtraction(data, n_examples, 1))    \n",
    "    y = oneHotEncoding(data[:, -1])\n",
    "    \n",
    "    n_features = np.size(x, 0)\n",
    "    \n",
    "    Theta1 = np.zeros([n_features, neurons_Hidden])\n",
    "    Theta2 = np.zeros([neurons_Hidden, 1])\n",
    "\n",
    "    ######################################\n",
    "    # Part 2: Linear Regression Training #\n",
    "    ######################################\n",
    "    for epochs in range(max_epochs):\n",
    "        # Randomly shuffle the data\n",
    "        randomIndex = rnd.sample(range(n_examples), n_examples)\n",
    "        x = x[:, randomIndex]\n",
    "        y = y[:, randomIndex]\n",
    "        \n",
    "        print(feedFoward(x,Theta1,Theta2))\n",
    "        break\n",
    "        \n",
    "        # Perform Gradient Descent\n",
    "        for k in range(n_classes):\n",
    "            for j in range(n_features):\n",
    "                for m in range(n_examples):\n",
    "                    if (j >= 1):\n",
    "                        theta[j,k] = theta[j,k] - (alfa/n_examples) * ((error[k,m] * x[j,m]) + (gamma * theta[j,k]))\n",
    "                    else:\n",
    "                        theta[j,k] = theta[j,k] - (alfa/n_examples) * (error[k,m] * x[j,m])\n",
    "\n",
    "        # Prints training status at each 100 epochs\n",
    "        if(epochs % 50 == 0):\n",
    "            print(\"###### Epoch\", epochs, \"######\")\n",
    "            print(\"Error:\", error_hist[epochs])\n",
    "            print(\"Thetas:\\n\", theta)\n",
    "            print(\"\")\n",
    "        \n",
    "        # Evaluate convergence and stops training if so\n",
    "        if(abs(error_hist[epochs] - error_hist[epochs-50]) <= epsilon):\n",
    "            print(\"Gradient Converged!!!\\nStopping at epoch\", epochs)\n",
    "            print(\"###### Epoch\", epochs, \"######\")\n",
    "            print(\"Error:\", error_hist[epochs])\n",
    "            print(\"Thetas:\\n\", theta)\n",
    "            print(\"\")\n",
    "            break\n",
    "\n",
    "    ######################################\n",
    "    # Part 3: Visualizing Classification #\n",
    "    ######################################\n",
    "    predictions = predictOHE(h_theta(x, Theta2))\n",
    "    \n",
    "    print(\"Results:\")\n",
    "    print(\"### Predictions ###\")\n",
    "    for i,j in zip(predictions, range(n_classes)):\n",
    "        print(\"\\nClass\",j)\n",
    "        print(i)\n",
    "        \n",
    "    print(\"\\n### Real ###\")\n",
    "    for i,j in zip(y, range(n_classes)):\n",
    "        print(\"\\nClass\",j)\n",
    "        print(i)\n",
    "#__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotagem dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Part 4: Data Plotting and Training Result #\n",
    "#############################################\n",
    "# First Figure: Dataset plotting\n",
    "iSetos = np.where(y[0,:] == 1)\n",
    "iVersi = np.where(y[1,:] == 1)\n",
    "iVirg = np.where(y[2,:] == 1)\n",
    "\n",
    "deciBound = np.array([decisionBound(theta[:,0], x), decisionBound(theta[:,1], x), decisionBound(theta[:,2], x)])\n",
    "\n",
    "plt.figure(2)\n",
    "plotCount = 1\n",
    "for i in range(1,4):\n",
    "    for j in range(i+1,5):\n",
    "        plt.subplot(2,3,plotCount)\n",
    "        plt.title(\"Iris Dataset Classification\\n(Green=Iris-setosa; Blue=Iris-versicolor; Red=Iris-virginica)\")\n",
    "        plt.grid()\n",
    "        plt.plot(x[i,iSetos], x[j,iSetos], 'go',\n",
    "                 x[i,iVersi], x[j,iVersi], 'bo',\n",
    "                 x[i,iVirg],  x[j,iVirg],  'ro',\n",
    "                 deciBound[0,0], deciBound[0,1], 'k-',\n",
    "                 deciBound[1,0], deciBound[1,1], 'k-')\n",
    "        plotCount += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"../datasets/dataMLP1.txt\")\n",
    "\n",
    "pos = np.where(data[:, 2] == 1)\n",
    "neg = np.where(data[:, 2] == 0)\n",
    "\n",
    "plt.figure(4)\n",
    "plt.plot(data[pos,1], data[pos,0], 'go', data[neg,1], data[neg,0], 'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
