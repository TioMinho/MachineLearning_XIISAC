{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Linear Simples\n",
    "\n",
    "### Descrição:\n",
    "Nesse notebook iremos desenvolver e executar um simples algoritmo de Regressão Linear. Utilizaremos um modelo linear simples, como já explicado, e um dataset de dados artificialmente gerados (para facilitar a visualização do funcionamento do algoritmo.\n",
    "\n",
    "A Regressão Linear é um algoritmo de Machine Learning (dos mais simples) que se propõe a predizer valores em um espaço contínuo. O produto de tal algoritmo é justamente um conjunto de parâmetros que definem um Modelo que determinamos. A aplicação de valores diretamente neste modelo, utilizando os parâmetros que foram \"aprendidos\" no treinamento, constituem uma predição de dados realizadas a partir desse treinamento.\n",
    "\n",
    "<b> Obs.: </b> todas as matrizes/vetores utilizados na fundamentação teórica são consideradas como Vetores-Colunas. A implementação pode diferir um pouco dessa convenção.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "%matplotlib qt5\n",
    "\n",
    "# Libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Linear\n",
    "\n",
    "Na célula abaixo, iremos programar a função responsável por calcular e retornar uma aplicação do nosso modelo linear:\n",
    "\n",
    "$$ h(\\theta) = \\theta_{0}+\\theta_{1}X_{1} $$\n",
    "\n",
    "Perceba que, por simplificação e otimização, podemos representar essa operação matricialmente da forma:\n",
    "\n",
    "$$ h(\\theta) = \\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1} \\end{bmatrix}^{T} \\times \\begin{bmatrix} 1 \\\\ X_{1} \\end{bmatrix} = \\begin{bmatrix} \\theta_{0} & \\theta_{1} \\end{bmatrix} \\times \\begin{bmatrix} 1 \\\\ X_{1} \\end{bmatrix}  = \\theta^{T}X $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [[11 13 23]]\n"
     ]
    }
   ],
   "source": [
    "# Definição da Função para o Modelo Linear\n",
    "def h_theta(x, theta):\n",
    "    ''' Apply the Linear Model for features X and parameters theta '''\n",
    "    return np.dot(np.transpose(theta), x)\n",
    "\n",
    "# Teste da Função: h([5;2]) = 5+2X\n",
    "testX = np.array([[1,1,1],\n",
    "                  [3,4,9]])\n",
    "\n",
    "testTheta = np.array([[5],\n",
    "                      [2]])\n",
    "\n",
    "print(\"Prediction:\", h_theta(testX, testTheta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de Custo\n",
    "\n",
    "Na célula abaixo, definimos uma função que calcula e retorna o custo total das nossas predições, isto é, o cálculo do erro total do treinamento. A função que utilizaremos pode ser arbitrária (apenas nos permite ter uma melhor visualização do treinamento, mas não influencia o mesmo).\n",
    "\n",
    "Nesse algoritmo, utilizaremos a Soma dos Resíduos Quadráticos (um dos seus muitos nomes):\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum (h(\\theta) - y)^{2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custo Total: 14.0\n"
     ]
    }
   ],
   "source": [
    "# Definição da Função de Erro\n",
    "def errorFunction(errors):\n",
    "    ''' Calculate the Least Square Error '''\n",
    "    return (1 / np.size(errors)) * np.sum(errors ** 2) \n",
    "\n",
    "# Teste da Função\n",
    "errors = np.array([5., 4., 4., 3., 2.])\n",
    "print(\"Custo Total:\", errorFunction(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programa Principal (Regressão Linear)\n",
    "\n",
    "No programa principal, iremos programar a Regressão Linear propriamente dita.\n",
    "Dividimos o código em três partes:\n",
    "\n",
    "### Part 1: Data Pre-Processing\n",
    "\n",
    "Nesse trecho, iremos nos preocupar em carregar e organizar o dataset que utilizaremos no treinamento. É nesse momento, também, que iremos declarar e separar as variáveis que definem o Conjunto de Atributos, o Conjunto de Saída e os Parâmetros do Modelo, além dos Hiperparâmetros de Treinamento. Iremos seguir a convenção de considerar todos os exemplares como vetores-colunas. No entanto, o numpy não nos permite facilmente modificar essa informação para o Conjunto de Saída, e o mesmo continuará como vetor-linha (sem muito prejuízo). Iremos criar, também um vetor para armazenar o Histórico de Erros do treinamento (por motivos de visualização).\n",
    "\n",
    "Iremos utilizar o dataset <i>data1.txt</i> localizado na pasta <i>datasets/</i>. Teremos as seguintes matrizes:\n",
    "\n",
    "$$\n",
    "    X = \\begin{bmatrix} 1 & 1 & \\cdots & 1 \\\\  X_{1}^{(1)} & X_{1}^{(2)} & \\cdots & X_{1}^{(m)}  \\end{bmatrix};\\   \\theta = \\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1}\\end{bmatrix};\\  Y = \\begin{bmatrix} Y^{(1)} & Y^{(2)} & \\cdots & Y^{(m)} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Part 2: Linear Regression Training\n",
    "\n",
    "Para cada época até a convergência (ou até atingir o limite máximo definido pelo Hiperparâmetro) iremos realizar o Treinamento da Regressão Linear. Os passos serão os seguintes:\n",
    "\n",
    "1. Calculamos o vetor de predição \"Y_pred\", como resultado da predição do Modelo para os parâmetros daquela época;\n",
    "2. Utilizando \"Y_pred\", calculamos os erros de acordo com o a matriz real \"Y\";\n",
    "3. Concatenamos o Custo Total do erro calculado no Histórico de Erros;\n",
    "4. Realizamos, para cada parâmetro, o Gradiente Descendente para estimar os novos valores dos parâmetros;\n",
    "5. Imprimimos os resultados do treino a cada 500 épocas;\n",
    "6. Verificamos uma possível convergência do treino, e paremos o mesmo caso seja verificado;\n",
    "\n",
    "### Part 3: Data Plotting and Training Results\n",
    "\n",
    "Ao fim do treinamento, iremos plotar duas figuras para avaliar o resultado final do nosso algoritmo. A <b>Figura 1</b> irá apenas exibir os atributos do Dataset. A <b>Figura 2</b> irá exibir a função estimada pelo nosso Modelo Linear, além do Histórico de Erros dado as épocas até convergência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Epoch 0 ######\n",
      "Error: 6341.89466667\n",
      "Thetas:\n",
      " [[ 0.39673333]\n",
      " [ 6.61924   ]]\n",
      "\n",
      "###### Epoch 500 ######\n",
      "Error: 18.9429261624\n",
      "Thetas:\n",
      " [[ 0.83402521]\n",
      " [ 4.72611325]]\n",
      "\n",
      "###### Epoch 1000 ######\n",
      "Error: 18.7129721891\n",
      "Thetas:\n",
      " [[ 1.36917352]\n",
      " [ 4.6941285 ]]\n",
      "\n",
      "###### Epoch 1500 ######\n",
      "Error: 18.4940619613\n",
      "Thetas:\n",
      " [[ 1.89131324]\n",
      " [ 4.66292124]]\n",
      "\n",
      "###### Epoch 2000 ######\n",
      "Error: 18.2856650932\n",
      "Thetas:\n",
      " [[ 2.40076058]\n",
      " [ 4.63247259]]\n",
      "\n",
      "###### Epoch 2500 ######\n",
      "Error: 18.0872766714\n",
      "Thetas:\n",
      " [[ 2.89782407]\n",
      " [ 4.60276409]]\n",
      "\n",
      "###### Epoch 3000 ######\n",
      "Error: 17.8984160312\n",
      "Thetas:\n",
      " [[ 3.38280474]\n",
      " [ 4.57377776]]\n",
      "\n",
      "###### Epoch 3500 ######\n",
      "Error: 17.7186255924\n",
      "Thetas:\n",
      " [[ 3.85599631]\n",
      " [ 4.54549604]]\n",
      "\n",
      "###### Epoch 4000 ######\n",
      "Error: 17.5474697504\n",
      "Thetas:\n",
      " [[ 4.31768535]\n",
      " [ 4.5179018 ]]\n",
      "\n",
      "###### Epoch 4500 ######\n",
      "Error: 17.384533821\n",
      "Thetas:\n",
      " [[ 4.76815147]\n",
      " [ 4.49097834]]\n",
      "\n",
      "###### Epoch 5000 ######\n",
      "Error: 17.2294230356\n",
      "Thetas:\n",
      " [[ 5.20766748]\n",
      " [ 4.46470934]]\n",
      "\n",
      "###### Epoch 5500 ######\n",
      "Error: 17.0817615845\n",
      "Thetas:\n",
      " [[ 5.63649956]\n",
      " [ 4.4390789 ]]\n",
      "\n",
      "###### Epoch 6000 ######\n",
      "Error: 16.9411917069\n",
      "Thetas:\n",
      " [[ 6.05490743]\n",
      " [ 4.41407149]]\n",
      "\n",
      "###### Epoch 6500 ######\n",
      "Error: 16.8073728235\n",
      "Thetas:\n",
      " [[ 6.46314446]\n",
      " [ 4.38967197]]\n",
      "\n",
      "###### Epoch 7000 ######\n",
      "Error: 16.6799807118\n",
      "Thetas:\n",
      " [[ 6.86145791]\n",
      " [ 4.36586557]]\n",
      "\n",
      "###### Epoch 7500 ######\n",
      "Error: 16.5587067205\n",
      "Thetas:\n",
      " [[ 7.250089  ]\n",
      " [ 4.34263786]]\n",
      "\n",
      "###### Epoch 8000 ######\n",
      "Error: 16.4432570211\n",
      "Thetas:\n",
      " [[ 7.62927309]\n",
      " [ 4.31997478]]\n",
      "\n",
      "###### Epoch 8500 ######\n",
      "Error: 16.333351897\n",
      "Thetas:\n",
      " [[ 7.99923982]\n",
      " [ 4.29786261]]\n",
      "\n",
      "###### Epoch 9000 ######\n",
      "Error: 16.2287250648\n",
      "Thetas:\n",
      " [[ 8.36021326]\n",
      " [ 4.27628794]]\n",
      "\n",
      "Gradient Converged!!!\n",
      "Stopping at epoch 9234\n",
      "###### Epoch 9234 ######\n",
      "Error: 16.181500763\n",
      "Thetas:\n",
      " [[ 8.5261215 ]\n",
      " [ 4.26637193]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Main Function\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    ###############################\n",
    "    # Part 1: Data Pre-Processing #\n",
    "    ###############################\n",
    "    # Loads the data\n",
    "    data = np.loadtxt(\"datasets/cricketData.txt\")\n",
    "    \n",
    "    n_examples = np.size(data,0)\n",
    "    n_features = np.size(data,1)\n",
    "    \n",
    "    # Define the model parameters\n",
    "    x = np.array([np.ones(n_examples), data[:, 0]])\n",
    "    y = data[:, 1]\n",
    "    theta = np.zeros([np.size(x, 0), 1])\n",
    "    \n",
    "    # Defines the hyperparameters and training measurements\n",
    "    alfa = 0.005\n",
    "    max_epochs = 500000\n",
    "    \n",
    "    error_hist = np.zeros([max_epochs])\n",
    "    epsilon = 0.01\n",
    "    \n",
    "    ######################################\n",
    "    # Part 2: Linear Regression Training #\n",
    "    ######################################\n",
    "    for epochs in range(max_epochs):\n",
    "        # Calculate the error vector from the current Model\n",
    "        y_pred = h_theta(x, theta)\n",
    "        error = y_pred - y\n",
    "\n",
    "        # Append new Least Square Error to History\n",
    "        error_hist[epochs] = errorFunction(error)\n",
    "\n",
    "        # Perform Gradient Descent\n",
    "        for j in range(n_features):\n",
    "            theta[j] = theta[j] - (alfa/n_examples) * np.sum(error * x[j,:])\n",
    "\n",
    "        # Prints training status at each 100 epochs\n",
    "        if(epochs % 500 == 0):\n",
    "            print(\"###### Epoch\", epochs, \"######\")\n",
    "            print(\"Error:\", error_hist[epochs])\n",
    "            print(\"Thetas:\\n\", theta)\n",
    "            print(\"\")\n",
    "        \n",
    "        # Evaluate convergence and stops training if so\n",
    "        if(abs(error_hist[epochs] - error_hist[epochs-50]) <= epsilon):\n",
    "            print(\"Gradient Converged!!!\\nStopping at epoch\", epochs)\n",
    "            print(\"###### Epoch\", epochs, \"######\")\n",
    "            print(\"Error:\", error_hist[epochs])\n",
    "            print(\"Thetas:\\n\", theta)\n",
    "            print(\"\")\n",
    "            break\n",
    "            \n",
    "    #############################################\n",
    "    # Part 3: Data Plotting and Training Result #\n",
    "    #############################################\n",
    "    # First Figure: Dataset plotting\n",
    "    plt.figure(1)\n",
    "    \n",
    "    plt.title(\"Influence of Temperature on Cricket Chirp Rate\")\n",
    "    plt.xlabel(\"Rate of Cricket Chirping\")\n",
    "    plt.ylabel(\"Temperature (ºF)\")\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.plot(x[1,:], y, 'rx')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Second Figure: Training results\n",
    "    plt.figure(2)\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Linear Regression Function Prediction\\n(Black=ModelPrediction)\")\n",
    "    plt.xlabel(\"Rate of Cricket Chirping\")\n",
    "    plt.ylabel(\"Temperature (ºF)\")\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.plot(x[1,:], y, 'rx', x[1,:], h_theta(x, theta)[0,:], 'k-')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(\"Error History\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Least Square Error\")\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.plot(error_hist[:epochs], \"g-\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "#__"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
